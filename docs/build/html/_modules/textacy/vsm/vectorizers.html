
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>textacy.vsm.vectorizers &#8212; textacy 0.9.1 documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for textacy.vsm.vectorizers</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Vectorizers</span>
<span class="sd">-----------</span>

<span class="sd">Transform a collection of tokenized documents into a document-term matrix</span>
<span class="sd">of shape (# docs, # unique terms), with various ways to filter or limit</span>
<span class="sd">included terms and flexible weighting schemes for their values.</span>

<span class="sd">A second option aggregates terms in tokenized documents by provided group labels,</span>
<span class="sd">resulting in a &quot;group-term-matrix&quot; of shape (# unique groups, # unique terms),</span>
<span class="sd">with filtering and weighting functionality as described above.</span>

<span class="sd">See the :class:`Vectorizer` and :class:`GroupVectorizer` docstrings for usage</span>
<span class="sd">examples and explanations of the various weighting schemes.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">array</span> <span class="k">import</span> <span class="n">array</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">normalize</span> <span class="k">as</span> <span class="n">normalize_mat</span>

<span class="kn">from</span> <span class="nn">.matrix_utils</span> <span class="k">import</span> <span class="n">get_doc_lengths</span><span class="p">,</span> <span class="n">get_inverse_doc_freqs</span><span class="p">,</span> <span class="n">filter_terms_by_df</span>


<span class="n">BM25_K1</span> <span class="o">=</span> <span class="mf">1.6</span>  <span class="c1"># value typically bounded in [1.2, 2.0]</span>
<span class="n">BM25_B</span> <span class="o">=</span> <span class="mf">0.75</span>


<div class="viewcode-block" id="Vectorizer"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer">[docs]</a><span class="k">class</span> <span class="nc">Vectorizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform one or more tokenized documents into a sparse document-term matrix</span>
<span class="sd">    of shape (# docs, # unique terms), with flexibly weighted and normalized values.</span>

<span class="sd">    Stream a corpus with metadata from disk:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; ds = textacy.datasets.CapitolWords()</span>
<span class="sd">        &gt;&gt;&gt; records = ds.records(limit=1000)</span>
<span class="sd">        &gt;&gt;&gt; corpus = textacy.Corpus(&quot;en&quot;, data=records)</span>
<span class="sd">        &gt;&gt;&gt; corpus</span>
<span class="sd">        Corpus(1000 docs; 538172 tokens)</span>

<span class="sd">    Tokenize and vectorize the first 600 documents of this corpus:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs = (</span>
<span class="sd">        ...     doc._.to_terms_list(ngrams=1, entities=True, as_strings=True)</span>
<span class="sd">        ...     for doc in corpus[:600])</span>
<span class="sd">        &gt;&gt;&gt; vectorizer = Vectorizer(</span>
<span class="sd">        ...     apply_idf=True, norm=&quot;l2&quot;,</span>
<span class="sd">        ...     min_df=3, max_df=0.95)</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = vectorizer.fit_transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix</span>
<span class="sd">        &lt;600x4346 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 69673 stored elements in Compressed Sparse Row format&gt;</span>

<span class="sd">    Tokenize and vectorize the remaining 400 documents of the corpus, using only</span>
<span class="sd">    the groups, terms, and weights learned in the previous step:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs = (</span>
<span class="sd">        ...     doc._.to_terms_list(ngrams=1, entities=True, as_strings=True)</span>
<span class="sd">        ...     for doc in corpus[600:])</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = vectorizer.transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix</span>
<span class="sd">        &lt;400x4346 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 38756 stored elements in Compressed Sparse Row format&gt;</span>

<span class="sd">    Inspect the terms associated with columns; they&#39;re sorted alphabetically:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; vectorizer.terms_list[:5]</span>
<span class="sd">        [&#39;&#39;, &#39;$&#39;, &#39;$ 1 million&#39;, &#39;$ 1.2 billion&#39;, &#39;$ 10 billion&#39;]</span>

<span class="sd">    (Btw: That empty string shouldn&#39;t be there. Somehow, spaCy is labeling it as</span>
<span class="sd">    a named entity...)</span>

<span class="sd">    If known in advance, limit the terms included in vectorized outputs</span>
<span class="sd">    to a particular set of values:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs = (</span>
<span class="sd">        ...     doc._.to_terms_list(ngrams=1, entities=True, as_strings=True)</span>
<span class="sd">        ...     for doc in corpus[:600])</span>
<span class="sd">        &gt;&gt;&gt; vectorizer = Vectorizer(</span>
<span class="sd">        ...     apply_idf=True, idf_type=&quot;smooth&quot;, norm=&quot;l2&quot;,</span>
<span class="sd">        ...     min_df=3, max_df=0.95,</span>
<span class="sd">        ...     vocabulary_terms=[&quot;president&quot;, &quot;bill&quot;, &quot;unanimous&quot;, &quot;distinguished&quot;, &quot;american&quot;])</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = vectorizer.fit_transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix</span>
<span class="sd">        &lt;600x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 844 stored elements in Compressed Sparse Row format&gt;</span>
<span class="sd">        &gt;&gt;&gt; vectorizer.terms_list</span>
<span class="sd">        [&#39;american&#39;, &#39;bill&#39;, &#39;distinguished&#39;, &#39;president&#39;, &#39;unanimous&#39;]</span>

<span class="sd">    Specify different weighting schemes to determine values in the matrix,</span>
<span class="sd">    adding or customizing individual components, as desired:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; money_idx = vectorizer.vocabulary_terms[&quot;$&quot;]</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = Vectorizer(</span>
<span class="sd">        ...     tf_type=&quot;linear&quot;, norm=None, min_df=3, max_df=0.95</span>
<span class="sd">        ...     ).fit_transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; print(doc_term_matrix[0:7, money_idx].toarray())</span>
<span class="sd">        [[0]</span>
<span class="sd">         [0]</span>
<span class="sd">         [1]</span>
<span class="sd">         [4]</span>
<span class="sd">         [0]</span>
<span class="sd">         [0]</span>
<span class="sd">         [2]]</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = Vectorizer(</span>
<span class="sd">        ...     tf_type=&quot;sqrt&quot;, apply_dl=True, dl_type=&quot;sqrt&quot;, norm=None, min_df=3, max_df=0.95</span>
<span class="sd">        ...     ).fit_transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; print(doc_term_matrix[0:7, money_idx].toarray())</span>
<span class="sd">        [[0.        ]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [0.10101525]</span>
<span class="sd">         [0.26037782]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [0.11396058]]</span>
<span class="sd">        &gt;&gt;&gt; doc_term_matrix = Vectorizer(</span>
<span class="sd">        ...     tf_type=&quot;bm25&quot;, apply_idf=True, idf_type=&quot;smooth&quot;, norm=None, min_df=3, max_df=0.95</span>
<span class="sd">        ...     ).fit_transform(tokenized_docs)</span>
<span class="sd">        &gt;&gt;&gt; print(doc_term_matrix[0:7, money_idx].toarray())</span>
<span class="sd">        [[0.        ]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [3.28353965]</span>
<span class="sd">         [5.82763722]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [0.        ]</span>
<span class="sd">         [4.83933924]]</span>

<span class="sd">    If you&#39;re not sure what&#39;s going on mathematically, :attr:`Vectorizer.weighting`</span>
<span class="sd">    gives the formula being used to calculate weights, based on the parameters</span>
<span class="sd">    set when initializing the vectorizer:</span>

<span class="sd">    .. code-block:: pycon</span>

<span class="sd">        &gt;&gt;&gt; vectorizer.weighting</span>
<span class="sd">        &#39;(tf * (k + 1)) / (k + tf) * log((n_docs + 1) / (df + 1)) + 1&#39;</span>

<span class="sd">    In general, weights may consist of a local component (term frequency),</span>
<span class="sd">    a global component (inverse document frequency), and a normalization</span>
<span class="sd">    component (document length). Individual components may be modified:</span>
<span class="sd">    they may have different scaling (e.g. tf vs. sqrt(tf)) or different behaviors</span>
<span class="sd">    (e.g. &quot;standard&quot; idf vs bm25&#39;s version). There are *many* possible weightings,</span>
<span class="sd">    and some may be better for particular use cases than others. When in doubt,</span>
<span class="sd">    though, just go with something standard.</span>

<span class="sd">    - &quot;tf&quot;: Weights are simply the absolute per-document term frequencies (tfs),</span>
<span class="sd">      i.e. value (i, j) in an output doc-term matrix corresponds to the number</span>
<span class="sd">      of occurrences of term j in doc i. Terms appearing many times in a given</span>
<span class="sd">      doc receive higher weights than less common terms.</span>
<span class="sd">      Params: ``tf_type=&quot;linear&quot;, apply_idf=False, apply_dl=False``</span>
<span class="sd">    - &quot;tfidf&quot;: Doc-specific, *local* tfs are multiplied by their corpus-wide,</span>
<span class="sd">      *global* inverse document frequencies (idfs). Terms appearing in many docs</span>
<span class="sd">      have higher document frequencies (dfs), correspondingly smaller idfs, and</span>
<span class="sd">      in turn, lower weights.</span>
<span class="sd">      Params: ``tf_type=&quot;linear&quot;, apply_idf=True, idf_type=&quot;smooth&quot;, apply_dl=False``</span>
<span class="sd">    - &quot;bm25&quot;: This scheme includes a local tf component that increases asymptotically,</span>
<span class="sd">      so higher tfs have diminishing effects on the overall weight; a global idf</span>
<span class="sd">      component that can go *negative* for terms that appear in a sufficiently</span>
<span class="sd">      high proportion of docs; as well as a row-wise normalization that accounts for</span>
<span class="sd">      document length, such that terms in shorter docs hit the tf asymptote sooner</span>
<span class="sd">      than those in longer docs.</span>
<span class="sd">      Params: ``tf_type=&quot;bm25&quot;, apply_idf=True, idf_type=&quot;bm25&quot;, apply_dl=True``</span>
<span class="sd">    - &quot;binary&quot;: This weighting scheme simply replaces all non-zero tfs with 1,</span>
<span class="sd">      indicating the presence or absence of a term in a particular doc. That&#39;s it.</span>
<span class="sd">      Params: ``tf_type=&quot;binary&quot;, apply_idf=False, apply_dl=False``</span>

<span class="sd">    Slightly altered versions of these &quot;standard&quot; weighting schemes are common,</span>
<span class="sd">    and may have better behavior in general use cases:</span>

<span class="sd">    - &quot;lucene-style tfidf&quot;: Adds a doc-length normalization to the usual local</span>
<span class="sd">      and global components.</span>
<span class="sd">      Params: ``tf_type=&quot;linear&quot;, apply_idf=True, idf_type=&quot;smooth&quot;, apply_dl=True, dl_type=&quot;sqrt&quot;``</span>
<span class="sd">    - &quot;lucene-style bm25&quot;: Uses a smoothed idf instead of the classic bm25 variant</span>
<span class="sd">      to prevent weights on terms from going negative.</span>
<span class="sd">      Params: ``tf_type=&quot;bm25&quot;, apply_idf=True, idf_type=&quot;smooth&quot;, apply_dl=True, dl_type=&quot;linear&quot;``</span>

<span class="sd">    Args:</span>
<span class="sd">        tf_type ({&quot;linear&quot;, &quot;sqrt&quot;, &quot;log&quot;, &quot;binary&quot;}): Type of term frequency (tf)</span>
<span class="sd">            to use for weights&#39; local component:</span>

<span class="sd">            - &quot;linear&quot;: tf (tfs are already linear, so left as-is)</span>
<span class="sd">            - &quot;sqrt&quot;: tf =&gt; sqrt(tf)</span>
<span class="sd">            - &quot;log&quot;: tf =&gt; log(tf) + 1</span>
<span class="sd">            - &quot;binary&quot;: tf =&gt; 1</span>

<span class="sd">        apply_idf (bool): If True, apply global idfs to local term weights, i.e.</span>
<span class="sd">            divide per-doc term frequencies by the (log of the) total number</span>
<span class="sd">            of documents in which they appear; otherwise, don&#39;t.</span>
<span class="sd">        idf_type ({&quot;standard&quot;, &quot;smooth&quot;, &quot;bm25&quot;}): Type of inverse document</span>
<span class="sd">            frequency (idf) to use for weights&#39; global component:</span>

<span class="sd">            - &quot;standard&quot;: idf = log(n_docs / df) + 1.0</span>
<span class="sd">            - &quot;smooth&quot;: idf = log(n_docs + 1 / df + 1) + 1.0, i.e. 1 is added</span>
<span class="sd">              to all document frequencies, as if a single document containing</span>
<span class="sd">              every unique term was added to the corpus. This prevents zero divisions!</span>
<span class="sd">            - &quot;bm25&quot;: idf = log((n_docs - df + 0.5) / (df + 0.5)), which is</span>
<span class="sd">              a form commonly used in information retrieval that allows for</span>
<span class="sd">              very common terms to receive negative weights.</span>

<span class="sd">        apply_dl (bool): If True, normalize local(+global) weights by doc length,</span>
<span class="sd">            i.e. divide by the total number of in-vocabulary terms appearing</span>
<span class="sd">            in a given doc; otherwise, don&#39;t.</span>
<span class="sd">        dl_type ({&quot;linear&quot;, &quot;sqrt&quot;, &quot;log&quot;}): Type of document-length scaling</span>
<span class="sd">            to use for weights&#39; normalization component:</span>

<span class="sd">            - &quot;linear&quot;: dl (dls are already linear, so left as-is)</span>
<span class="sd">            - &quot;sqrt&quot;: dl =&gt; sqrt(dl)</span>
<span class="sd">            - &quot;log&quot;: dl =&gt; log(dl)</span>

<span class="sd">        norm ({&quot;l1&quot;, &quot;l2&quot;} or None): If &quot;l1&quot; or &quot;l2&quot;, normalize weights by the</span>
<span class="sd">            L1 or L2 norms, respectively, of row-wise vectors; otherwise, don&#39;t.</span>
<span class="sd">        vocabulary_terms (Dict[str, int] or Iterable[str]): Mapping of unique term</span>
<span class="sd">            string to unique term id, or an iterable of term strings that gets</span>
<span class="sd">            converted into a suitable mapping. Note that, if specified, vectorized</span>
<span class="sd">            outputs will include *only* these terms as columns.</span>
<span class="sd">        min_df (float or int): If float, value is the fractional proportion of</span>
<span class="sd">            the total number of documents, which must be in [0.0, 1.0]. If int,</span>
<span class="sd">            value is the absolute number. Filter terms whose document frequency</span>
<span class="sd">            is less than ``min_df``.</span>
<span class="sd">        max_df (float or int): If float, value is the fractional proportion of</span>
<span class="sd">            the total number of documents, which must be in [0.0, 1.0]. If int,</span>
<span class="sd">            value is the absolute number. Filter terms whose document frequency</span>
<span class="sd">            is greater than ``max_df``.</span>
<span class="sd">        max_n_terms (int): Only include terms whose document frequency is within</span>
<span class="sd">            the top ``max_n_terms``.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        vocabulary_terms (Dict[str, int]): Mapping of unique term string to unique</span>
<span class="sd">            term id, either provided on instantiation or generated by calling</span>
<span class="sd">            :meth:`Vectorizer.fit()` on a collection of tokenized documents.</span>
<span class="sd">        id_to_term (Dict[int, str]): Mapping of unique term id to unique term</span>
<span class="sd">            string, i.e. the inverse of :attr:`Vectorizer.vocabulary_terms`.</span>
<span class="sd">            This mapping is only generated as needed.</span>
<span class="sd">        terms_list (List[str]): List of term strings in column order of</span>
<span class="sd">            vectorized outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">apply_idf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span>
        <span class="n">apply_dl</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dl_type</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span>
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">max_df</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">max_n_terms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary_terms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># sanity check numeric arguments</span>
        <span class="k">if</span> <span class="n">min_df</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">max_df</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`min_df` and `max_df` must be positive numbers or None&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_n_terms</span> <span class="ow">and</span> <span class="n">max_n_terms</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`max_n_terms` must be a positive integer or None&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">=</span> <span class="n">tf_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_idf</span> <span class="o">=</span> <span class="n">apply_idf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idf_type</span> <span class="o">=</span> <span class="n">idf_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="o">=</span> <span class="n">apply_dl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span> <span class="o">=</span> <span class="n">dl_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_df</span> <span class="o">=</span> <span class="n">min_df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_df</span> <span class="o">=</span> <span class="n">max_df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_n_terms</span> <span class="o">=</span> <span class="n">max_n_terms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_vocabulary</span><span class="p">(</span>
            <span class="n">vocabulary_terms</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">id_to_term_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_avg_doc_length</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_validate_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validate an input vocabulary. If it&#39;s a mapping, ensure that term ids</span>
<span class="sd">        are unique and compact (i.e. without any gaps between 0 and the number</span>
<span class="sd">        of terms in ``vocabulary``. If it&#39;s a sequence, sort terms then assign</span>
<span class="sd">        integer ids in ascending order.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocabulary_terms (Dict[str, int] or Iterable[str])</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, int]</span>
<span class="sd">            bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">vocabulary</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span><span class="p">):</span>
                <span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">term</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">vocab</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s1">&#39;Terms in `vocabulary` must be unique, but &quot;</span><span class="si">{}</span><span class="s1">&quot; &#39;</span>
                            <span class="s2">&quot;was found more than once.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
                        <span class="p">)</span>
                <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocab</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">):</span>
                    <span class="n">counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
                    <span class="n">n_dupe_term_ids</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                        <span class="mi">1</span>
                        <span class="k">for</span> <span class="n">term_id</span><span class="p">,</span> <span class="n">term_id_count</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">term_id_count</span> <span class="o">&gt;</span> <span class="mi">1</span>
                    <span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Term ids in `vocabulary` must be unique, but </span><span class="si">{}</span><span class="s2"> ids&quot;</span>
                        <span class="s2">&quot;were assigned to more than one term.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_dupe_term_ids</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Term ids in `vocabulary` must be compact, i.e. &quot;</span>
                            <span class="s2">&quot;not have any gaps, but term id </span><span class="si">{}</span><span class="s2"> is missing from &quot;</span>
                            <span class="s2">&quot;a vocabulary of </span><span class="si">{}</span><span class="s2"> terms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>
                        <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">vocabulary</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`vocabulary` must not be empty.&quot;</span><span class="p">)</span>
            <span class="n">is_fixed</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">is_fixed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">is_fixed</span>

    <span class="k">def</span> <span class="nf">_check_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check that instance has a valid vocabulary mapping;</span>
<span class="sd">        if not, raise a ValueError.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;vocabulary hasn&#39;t been built; call `Vectorizer.fit()`&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;vocabulary is empty&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">id_to_term</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        dict: Mapping of unique term id (int) to unique term string (str), i.e.</span>
<span class="sd">            the inverse of :attr:`Vectorizer.vocabulary`. This attribute is only</span>
<span class="sd">            generated if needed, and it is automatically kept in sync with the</span>
<span class="sd">            corresponding vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">id_to_term_</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">id_to_term_</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">term_id</span><span class="p">:</span> <span class="n">term_str</span> <span class="k">for</span> <span class="n">term_str</span><span class="p">,</span> <span class="n">term_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_to_term_</span>

    <span class="c1"># TODO: Do we *want* to allow setting to this property?</span>
    <span class="c1"># @id_to_term.setter</span>
    <span class="c1"># def id_to_term(self, new_id_to_term):</span>
    <span class="c1">#     self.id_to_term_ = new_id_to_term</span>
    <span class="c1">#     self.vocabulary_terms = {</span>
    <span class="c1">#         term_str: term_id for term_id, term_str in new_id_to_term.items()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">terms_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        List of term strings in column order of vectorized outputs. For example,</span>
<span class="sd">        ``terms_list[0]`` gives the term assigned to the first column in an</span>
<span class="sd">        output doc-term-matrix, ``doc_term_matrix[:, 0]``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">term_str</span>
            <span class="k">for</span> <span class="n">term_str</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">]</span>

<div class="viewcode-block" id="Vectorizer.fit"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms in ``tokenized_docs`` and, if not already provided, build up</span>
<span class="sd">        a vocabulary based those terms. Fit and store global weights (IDFs)</span>
<span class="sd">        and, if needed for term weighting, the average document length.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`Vectorizer`: The instance that has just been fit.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Vectorizer.fit_transform"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer.fit_transform">[docs]</a>    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms in ``tokenized_docs`` and, if not already provided, build up</span>
<span class="sd">        a vocabulary based those terms. Fit and store global weights (IDFs)</span>
<span class="sd">        and, if needed for term weighting, the average document length.</span>
<span class="sd">        Transform ``tokenized_docs`` into a document-term matrix with values</span>
<span class="sd">        weighted according to the parameters in :class:`Vectorizer` initialization.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`: The transformed document-term matrix.</span>
<span class="sd">            Rows correspond to documents and columns correspond to terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># count terms and fit global weights</span>
        <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
        <span class="c1"># re-weight values in doc-term matrix, as specified in init</span>
        <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reweight_values</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">doc_term_matrix</span></div>

<div class="viewcode-block" id="Vectorizer.transform"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer.transform">[docs]</a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform ``tokenized_docs`` into a document-term matrix with values</span>
<span class="sd">        weighted according to the parameters in :class:`Vectorizer` initialization</span>
<span class="sd">        and the global weights computed by calling :meth:`Vectorizer.fit()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`: The transformed document-term matrix.</span>
<span class="sd">            Rows correspond to documents and columns correspond to terms.</span>

<span class="sd">        Note:</span>
<span class="sd">            For best results, the tokenization used to produce ``tokenized_docs``</span>
<span class="sd">            should be the same as was applied to the docs used in fitting this</span>
<span class="sd">            vectorizer or in generating a fixed input vocabulary.</span>

<span class="sd">            Consider an extreme case where the docs used in fitting consist of</span>
<span class="sd">            lowercased (non-numeric) terms, while the docs to be transformed are</span>
<span class="sd">            all uppercased: The output doc-term-matrix will be empty.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>
        <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_terms</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reweight_values</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms and, if :attr:`Vectorizer.fixed_terms` is False, build up</span>
<span class="sd">        a vocabulary based on the terms found in ``tokenized_docs``. Transform</span>
<span class="sd">        ``tokenized_docs`` into a document-term matrix with absolute tf weights.</span>
<span class="sd">        Store global weights (IDFs) and, if :attr:`Vectorizer.doc_length_norm`</span>
<span class="sd">        is not None, the average doc length.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]])</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># count terms and, if not provided on init, build up a vocabulary</span>
        <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_terms</span><span class="p">(</span>
            <span class="n">tokenized_docs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># filter terms by doc freq or info content, as specified in init</span>
            <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_terms</span><span class="p">(</span>
                <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span>
            <span class="p">)</span>
            <span class="c1"># sort features alphabetically (vocabulary_terms modified in-place)</span>
            <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_vocab_and_matrix</span><span class="p">(</span>
                <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span>
            <span class="p">)</span>
            <span class="c1"># *now* vocabulary_terms are known and fixed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="n">vocabulary_terms</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">n_docs</span><span class="p">,</span> <span class="n">n_terms</span> <span class="o">=</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_idf</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># store the global weights as a diagonal sparse matrix of idfs</span>
            <span class="n">idfs</span> <span class="o">=</span> <span class="n">get_inverse_doc_freqs</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">idf_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">spdiags</span><span class="p">(</span>
                <span class="n">idfs</span><span class="p">,</span> <span class="n">diags</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">n_terms</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_terms</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;bm25&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># store the avg document length, used in bm25 weighting to normalize</span>
            <span class="c1"># term weights by the length of the containing documents</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_avg_doc_length</span> <span class="o">=</span> <span class="n">get_doc_lengths</span><span class="p">(</span>
                <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span>
            <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">doc_term_matrix</span>

    <span class="k">def</span> <span class="nf">_count_terms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">fixed_vocab</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms found in ``tokenized_docs`` and, if ``fixed_vocab`` is False,</span>
<span class="sd">        build up a vocabulary based on those terms.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]])</span>
<span class="sd">            fixed_vocab (bool)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">            Dict[str, int]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fixed_vocab</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># add a new value when a new term is seen</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">()</span>
            <span class="n">vocabulary</span><span class="o">.</span><span class="n">default_factory</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="o">.</span><span class="fm">__len__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">))</span>
        <span class="n">indptr</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">terms</span> <span class="ow">in</span> <span class="n">tokenized_docs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">terms</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">term</span><span class="p">])</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="c1"># ignore out-of-vocabulary terms when _fixed_terms=True</span>
                    <span class="k">continue</span>
            <span class="n">indptr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">fixed_vocab</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># we no longer want defaultdict behaviour</span>
            <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>
        <span class="n">indptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>

        <span class="c1"># build the matrix, then consolidate duplicate entries</span>
        <span class="c1"># by adding them together, in-place</span>
        <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span>
            <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">indptr</span><span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indptr</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">sum_duplicates</span><span class="p">()</span>

        <span class="c1"># pretty sure this is a good thing to do... o_O</span>
        <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary</span>

    <span class="k">def</span> <span class="nf">_filter_terms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Filter terms in ``vocabulary`` by their document frequency or information</span>
<span class="sd">        content, as specified in :class:`Vectorizer` initialization.</span>

<span class="sd">        Args:</span>
<span class="sd">            doc_term_matrix (:class:`sp.sparse.csr_matrix`)</span>
<span class="sd">            vocabulary (Dict[str, int])</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">            Dict[str, int]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_df</span> <span class="o">!=</span> <span class="mf">1.0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_df</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n_terms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">filter_terms_by_df</span><span class="p">(</span>
                <span class="n">doc_term_matrix</span><span class="p">,</span>
                <span class="n">vocabulary</span><span class="p">,</span>
                <span class="n">max_df</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_df</span><span class="p">,</span>
                <span class="n">min_df</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_df</span><span class="p">,</span>
                <span class="n">max_n_terms</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_n_terms</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vocabulary</span>

    <span class="k">def</span> <span class="nf">_sort_vocab_and_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sort terms in ``vocabulary`` alphabetically, modifying the vocabulary</span>
<span class="sd">        in-place, and returning a correspondingly reordered ``matrix`` along</span>
<span class="sd">        its rows or columns, depending on ``axis``.</span>

<span class="sd">        Args:</span>
<span class="sd">            matrix (:class:`sp.sparse.csr_matrix`)</span>
<span class="sd">            vocabulary (Dict[str, int])</span>
<span class="sd">            axis ({&#39;rows&#39;, &#39;columns&#39;} or {0, 1})</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sorted_vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="n">new_idx_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sorted_vocab</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">new_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">old_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_vocab</span><span class="p">):</span>
            <span class="n">new_idx_array</span><span class="p">[</span><span class="n">new_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_idx</span>
            <span class="n">vocabulary</span><span class="p">[</span><span class="n">term</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_idx</span>
        <span class="c1"># use fancy indexing to reorder rows or columns</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="s2">&quot;rows&quot;</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">matrix</span><span class="p">[</span><span class="n">new_idx_array</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="s2">&quot;columns&quot;</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">matrix</span><span class="p">[:,</span> <span class="n">new_idx_array</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`axis` = </span><span class="si">{}</span><span class="s2"> is invalid; must be one of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">axis</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;rows&quot;</span><span class="p">,</span> <span class="s2">&quot;columns&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reweight_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Re-weight values in a doc-term matrix according to parameters specified</span>
<span class="sd">        in :class:`Vectorizer` initialization: binary or tf-idf weighting,</span>
<span class="sd">        sublinear term-frequency, document-normalized weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            doc_term_matrix (:class:`sp.sparse.csr_matrix`)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># re-weight the local components (term freqs)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;binary&quot;</span><span class="p">:</span>
            <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;bm25&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">BM25_K1</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
                    <span class="o">/</span> <span class="p">(</span><span class="n">BM25_K1</span> <span class="o">+</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dls</span> <span class="o">=</span> <span class="n">get_doc_lengths</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span><span class="p">)</span>
                <span class="n">length_norm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">BM25_B</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">BM25_B</span> <span class="o">*</span> <span class="p">(</span><span class="n">dls</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_avg_doc_length</span><span class="p">))</span>
                <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">tocoo</span><span class="p">(</span><span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">BM25_K1</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
                    <span class="o">/</span> <span class="p">(</span>
                        <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span>
                        <span class="o">+</span> <span class="p">(</span><span class="n">BM25_K1</span> <span class="o">*</span> <span class="n">length_norm</span><span class="p">[</span><span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">row</span><span class="p">])</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(</span><span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;unsafe&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">casting</span><span class="o">=</span><span class="s2">&quot;unsafe&quot;</span><span class="p">)</span>
            <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">1.0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">pass</span>  <span class="c1"># tfs are already linear</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># this should never raise, i&#39;m just being a worrywart</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`tf_type` = </span><span class="si">{}</span><span class="s2"> is invalid&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span><span class="p">))</span>

        <span class="c1"># apply the global component (idfs), column-wise</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_idf</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">doc_term_matrix</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span>

        <span class="c1"># apply normalizations, row-wise</span>
        <span class="c1"># unless we&#39;ve already handled it for bm25-style tf</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">!=</span> <span class="s2">&quot;bm25&quot;</span><span class="p">:</span>
            <span class="n">n_docs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">dls</span> <span class="o">=</span> <span class="n">get_doc_lengths</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span><span class="p">)</span>
            <span class="n">dl_diag</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">spdiags</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">dls</span><span class="p">,</span> <span class="n">diags</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">n_docs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_docs</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">)</span>
            <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">dl_diag</span> <span class="o">*</span> <span class="n">doc_term_matrix</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">normalize_mat</span><span class="p">(</span>
                <span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">doc_term_matrix</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weighting</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        str: A mathematical representation of the overall weighting scheme</span>
<span class="sd">        used to determine values in the vectorized matrix, depending on the</span>
<span class="sd">        params used to initialize the :class:`Vectorizer`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">w</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tf_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;binary&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="s2">&quot;tf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span> <span class="s2">&quot;sqrt(tf)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="s2">&quot;log(tf)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;bm25&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="kc">True</span><span class="p">:</span> <span class="s2">&quot;(tf * (k + 1)) / (tf + k * (1 - b + b * (length / avg(lengths)))&quot;</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;(tf * (k + 1)) / (tf + k)&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
        <span class="n">idf_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;standard&quot;</span><span class="p">:</span> <span class="s2">&quot;log(n_docs / df) + 1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;smooth&quot;</span><span class="p">:</span> <span class="s2">&quot;log((n_docs + 1) / (df + 1)) + 1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;bm25&quot;</span><span class="p">:</span> <span class="s2">&quot;log((n_docs - df + 0.5) / (df + 0.5))&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">dl_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="s2">&quot;1/length&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span> <span class="s2">&quot;1/sqrt(length)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="s2">&quot;1/log(length) + 1&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;bm25&quot;</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_idf</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idf_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idf_type</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">!=</span> <span class="s2">&quot;bm25&quot;</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dl_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span><span class="p">])</span>
        <span class="k">return</span> <span class="s2">&quot; * &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="p">)</span></div>


<div class="viewcode-block" id="GroupVectorizer"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.GroupVectorizer">[docs]</a><span class="k">class</span> <span class="nc">GroupVectorizer</span><span class="p">(</span><span class="n">Vectorizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform one or more tokenized documents into a group-term matrix of</span>
<span class="sd">    shape (# groups, # unique terms), with tf-, tf-idf, or binary-weighted values.</span>

<span class="sd">    This is an extension of typical document-term matrix vectorization, where</span>
<span class="sd">    terms are grouped by the documents in which they co-occur. It allows for</span>
<span class="sd">    customized grouping, such as by a shared author or publication year, that</span>
<span class="sd">    may span multiple documents, without forcing users to merge those documents</span>
<span class="sd">    themselves.</span>

<span class="sd">    Stream a corpus with metadata from disk::</span>

<span class="sd">        &gt;&gt;&gt; ds = textacy.datasets.CapitolWords()</span>
<span class="sd">        &gt;&gt;&gt; records = ds.records(limit=1000)</span>
<span class="sd">        &gt;&gt;&gt; corpus = textacy.Corpus(&quot;en&quot;, data=records)</span>
<span class="sd">        &gt;&gt;&gt; corpus</span>
<span class="sd">        Corpus(1000 docs; 538172 tokens)</span>

<span class="sd">    Tokenize and vectorize the first 600 documents of this corpus, where terms</span>
<span class="sd">    are grouped not by documents but by a categorical value in the docs&#39; metadata::</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs, groups = textacy.io.unzip(</span>
<span class="sd">        ...     (doc._.to_terms_list(ngrams=1, entities=True, as_strings=True),</span>
<span class="sd">        ...      doc._.meta[&quot;speaker_name&quot;])</span>
<span class="sd">        ...     for doc in corpus[:600])</span>
<span class="sd">        &gt;&gt;&gt; vectorizer = GroupVectorizer(</span>
<span class="sd">        ...     apply_idf=True, idf_type=&quot;smooth&quot;, norm=&quot;l2&quot;,</span>
<span class="sd">        ...     min_df=3, max_df=0.95)</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix</span>
<span class="sd">        &lt;5x1793 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 6075 stored elements in Compressed Sparse Row format&gt;</span>

<span class="sd">    Tokenize and vectorize the remaining 400 documents of the corpus, using only</span>
<span class="sd">    the groups, terms, and weights learned in the previous step::</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs, groups = textacy.io.unzip(</span>
<span class="sd">        ...     (doc._.to_terms_list(ngrams=1, entities=True, as_strings=True),</span>
<span class="sd">        ...      doc._.meta[&quot;speaker_name&quot;])</span>
<span class="sd">        ...     for doc in corpus[600:])</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix = vectorizer.transform(tokenized_docs, groups)</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix</span>
<span class="sd">        &lt;5x1793 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 4440 stored elements in Compressed Sparse Row format&gt;</span>

<span class="sd">    Inspect the terms associated with columns and groups associated with rows;</span>
<span class="sd">    they&#39;re sorted alphabetically::</span>

<span class="sd">        &gt;&gt;&gt; vectorizer.terms_list[:5]</span>
<span class="sd">        [&#39;$ 1 million&#39;, &#39;$ 160 million&#39;, &#39;$ 7 billion&#39;, &#39;0&#39;, &#39;1 minute&#39;]</span>
<span class="sd">        &gt;&gt;&gt; vectorizer.grps_list</span>
<span class="sd">        [&#39;Bernie Sanders&#39;, &#39;John Kasich&#39;, &#39;Joseph Biden&#39;, &#39;Lindsey Graham&#39;, &#39;Rick Santorum&#39;]</span>

<span class="sd">    If known in advance, limit the terms and/or groups included in vectorized outputs</span>
<span class="sd">    to a particular set of values::</span>

<span class="sd">        &gt;&gt;&gt; tokenized_docs, groups = textacy.io.unzip(</span>
<span class="sd">        ...     (doc._.to_terms_list(ngrams=1, entities=True, as_strings=True),</span>
<span class="sd">        ...      doc._.meta[&quot;speaker_name&quot;])</span>
<span class="sd">        ...     for doc in corpus[:600])</span>
<span class="sd">        &gt;&gt;&gt; vectorizer = GroupVectorizer(</span>
<span class="sd">        ...     apply_idf=True, idf_type=&quot;smooth&quot;, norm=&quot;l2&quot;,</span>
<span class="sd">        ...     min_df=3, max_df=0.95,</span>
<span class="sd">        ...     vocabulary_terms=[&quot;legislation&quot;, &quot;federal government&quot;, &quot;house&quot;, &quot;constitutional&quot;],</span>
<span class="sd">        ...     vocabulary_grps=[&quot;Bernie Sanders&quot;, &quot;Lindsey Graham&quot;, &quot;Rick Santorum&quot;])</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)</span>
<span class="sd">        &gt;&gt;&gt; grp_term_matrix</span>
<span class="sd">        &lt;3x4 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="sd">                with 12 stored elements in Compressed Sparse Row format&gt;</span>
<span class="sd">        &gt;&gt;&gt; vectorizer.terms_list</span>
<span class="sd">        [&#39;constitutional&#39;, &#39;federal government&#39;, &#39;house&#39;, &#39;legislation&#39;]</span>
<span class="sd">        &gt;&gt;&gt; vectorizer.grps_list</span>
<span class="sd">        [&#39;Bernie Sanders&#39;, &#39;Lindsey Graham&#39;, &#39;Rick Santorum&#39;]</span>

<span class="sd">    For a discussion of the various weighting schemes that can be applied, check</span>
<span class="sd">    out the :class:`Vectorizer` docstring.</span>

<span class="sd">    Args:</span>
<span class="sd">        tf_type ({&quot;linear&quot;, &quot;sqrt&quot;, &quot;log&quot;, &quot;binary&quot;}): Type of term frequency (tf)</span>
<span class="sd">            to use for weights&#39; local component:</span>

<span class="sd">            - &quot;linear&quot;: tf (tfs are already linear, so left as-is)</span>
<span class="sd">            - &quot;sqrt&quot;: tf =&gt; sqrt(tf)</span>
<span class="sd">            - &quot;log&quot;: tf =&gt; log(tf) + 1</span>
<span class="sd">            - &quot;binary&quot;: tf =&gt; 1</span>

<span class="sd">        apply_idf (bool): If True, apply global idfs to local term weights, i.e.</span>
<span class="sd">            divide per-doc term frequencies by the total number of documents</span>
<span class="sd">            in which they appear (well, the log of that number); otherwise, don&#39;t.</span>
<span class="sd">        idf_type ({&quot;standard&quot;, &quot;smooth&quot;, &quot;bm25&quot;}): Type of inverse document</span>
<span class="sd">            frequency (idf) to use for weights&#39; global component:</span>

<span class="sd">            - &quot;standard&quot;: idf = log(n_docs / df) + 1.0</span>
<span class="sd">            - &quot;smooth&quot;: idf = log(n_docs + 1 / df + 1) + 1.0, i.e. 1 is added</span>
<span class="sd">              to all document frequencies, as if a single document containing</span>
<span class="sd">              every unique term was added to the corpus.</span>
<span class="sd">            - &quot;bm25&quot;: idf = log((n_docs - df + 0.5) / (df + 0.5)), which is</span>
<span class="sd">              a form commonly used in information retrieval that allows for</span>
<span class="sd">              very common terms to receive negative weights.</span>

<span class="sd">        apply_dl (bool): If True, normalize local(+global) weights by doc length,</span>
<span class="sd">            i.e. divide by the total number of in-vocabulary terms appearing</span>
<span class="sd">            in a given doc; otherwise, don&#39;t.</span>
<span class="sd">        dl_type ({&quot;linear&quot;, &quot;sqrt&quot;, &quot;log&quot;}): Type of document-length scaling</span>
<span class="sd">            to use for weights&#39; normalization component:</span>

<span class="sd">            - &quot;linear&quot;: dl (dls are already linear, so left as-is)</span>
<span class="sd">            - &quot;sqrt&quot;: dl =&gt; sqrt(dl)</span>
<span class="sd">            - &quot;log&quot;: dl =&gt; log(dl)</span>

<span class="sd">        norm ({&quot;l1&quot;, &quot;l2&quot;} or None): If &quot;l1&quot; or &quot;l2&quot;, normalize weights by the</span>
<span class="sd">            L1 or L2 norms, respectively, of row-wise vectors; otherwise, don&#39;t.</span>
<span class="sd">        vocabulary_terms (Dict[str, int] or Iterable[str]): Mapping of unique term</span>
<span class="sd">            string to unique term id, or an iterable of term strings that gets</span>
<span class="sd">            converted into a suitable mapping. Note that, if specified, vectorized</span>
<span class="sd">            outputs will include *only* these terms as columns.</span>
<span class="sd">        vocabulary_grps (Dict[str, int] or Iterable[str]): Mapping of unique group</span>
<span class="sd">            string to unique group id, or an iterable of group strings that gets</span>
<span class="sd">            converted into a suitable mapping. Note that, if specified, vectorized</span>
<span class="sd">            outputs will include *only* these groups as rows.</span>
<span class="sd">        min_df (float or int): If float, value is the fractional proportion of</span>
<span class="sd">            the total number of documents, which must be in [0.0, 1.0]. If int,</span>
<span class="sd">            value is the absolute number. Filter terms whose document frequency</span>
<span class="sd">            is less than ``min_df``.</span>
<span class="sd">        max_df (float or int): If float, value is the fractional proportion of</span>
<span class="sd">            the total number of documents, which must be in [0.0, 1.0]. If int,</span>
<span class="sd">            value is the absolute number. Filter terms whose document frequency</span>
<span class="sd">            is greater than ``max_df``.</span>
<span class="sd">        max_n_terms (int): Only include terms whose document frequency is within</span>
<span class="sd">            the top ``max_n_terms``.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        vocabulary_terms (Dict[str, int]): Mapping of unique term string to unique</span>
<span class="sd">            term id, either provided on instantiation or generated by calling</span>
<span class="sd">            :meth:`GroupVectorizer.fit()` on a collection of tokenized documents.</span>
<span class="sd">        vocabulary_grps (Dict[str, int]): Mapping of unique group string to unique</span>
<span class="sd">            group id, either provided on instantiation or generated by calling</span>
<span class="sd">            :meth:`GroupVectorizer.fit()` on a collection of tokenized documents.</span>
<span class="sd">        id_to_term (Dict[int, str]): Mapping of unique term id to unique term</span>
<span class="sd">            string, i.e. the inverse of :attr:`GroupVectorizer.vocabulary_terms`.</span>
<span class="sd">            This mapping is only generated as needed.</span>
<span class="sd">        id_to_grp (Dict[int, str]): Mapping of unique group id to unique group</span>
<span class="sd">            string, i.e. the inverse of :attr:`GroupVectorizer.vocabulary_grps`.</span>
<span class="sd">            This mapping is only generated as needed.</span>
<span class="sd">        terms_list (List[str]): List of term strings in column order of</span>
<span class="sd">            vectorized outputs.</span>
<span class="sd">        grps_list (List[str]): List of group strings in row order of</span>
<span class="sd">            vectorized outputs.</span>

<span class="sd">    See Also:</span>
<span class="sd">        :class:`Vectorizer`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">apply_idf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span>
        <span class="n">apply_dl</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dl_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">max_df</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">max_n_terms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary_terms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">vocabulary_grps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">tf_type</span><span class="o">=</span><span class="n">tf_type</span><span class="p">,</span>
            <span class="n">apply_idf</span><span class="o">=</span><span class="n">apply_idf</span><span class="p">,</span>
            <span class="n">idf_type</span><span class="o">=</span><span class="n">idf_type</span><span class="p">,</span>
            <span class="n">apply_dl</span><span class="o">=</span><span class="n">apply_dl</span><span class="p">,</span>
            <span class="n">dl_type</span><span class="o">=</span><span class="n">dl_type</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
            <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span>
            <span class="n">max_df</span><span class="o">=</span><span class="n">max_df</span><span class="p">,</span>
            <span class="n">max_n_terms</span><span class="o">=</span><span class="n">max_n_terms</span><span class="p">,</span>
            <span class="n">vocabulary_terms</span><span class="o">=</span><span class="n">vocabulary_terms</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># now do the same thing for grps as was done for terms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_grps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_vocabulary</span><span class="p">(</span>
            <span class="n">vocabulary_grps</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">id_to_grp_</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">id_to_grp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        dict: Mapping of unique group id (int) to unique group string (str), i.e.</span>
<span class="sd">            the inverse of :attr:`GroupVectorizer.vocabulary_grps`. This attribute</span>
<span class="sd">            is only generated if needed, and it is automatically kept in sync</span>
<span class="sd">            with the corresponding vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">id_to_grp_</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">id_to_grp_</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">grp_id</span><span class="p">:</span> <span class="n">grp_str</span> <span class="k">for</span> <span class="n">grp_str</span><span class="p">,</span> <span class="n">grp_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_to_grp_</span>

    <span class="c1"># @id_to_grp.setter</span>
    <span class="c1"># def id_to_grp(self, new_id_to_grp):</span>
    <span class="c1">#     self.id_to_grp_ = new_id_to_grp</span>
    <span class="c1">#     self.vocabulary_grps = {</span>
    <span class="c1">#         grp_str: grp_id for grp_id, grp_str in new_id_to_grp.items()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">grps_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        List of group strings in row order of vectorized outputs. For example,</span>
<span class="sd">        ``grps_list[0]`` gives the group assigned to the first row in an</span>
<span class="sd">        output group-term-matrix, ``grp_term_matrix[0, :]``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">grp_str</span>
            <span class="k">for</span> <span class="n">grp_str</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">]</span>

<div class="viewcode-block" id="GroupVectorizer.fit"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.GroupVectorizer.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms in ``tokenized_docs`` and, if not already provided, build up</span>
<span class="sd">        a vocabulary based those terms; do the same for the groups in ``grps``.</span>
<span class="sd">        Fit and store global weights (IDFs) and, if needed for term weighting,</span>
<span class="sd">        the average document length.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">            grps (Iterable[str]): Sequence of group names by which the terms in</span>
<span class="sd">                ``tokenized_docs`` are aggregated, where the first item in ``grps``</span>
<span class="sd">                corresponds to the first item in ``tokenized_docs``, and so on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`GroupVectorizer`: The instance that has just been fit.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="GroupVectorizer.fit_transform"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.GroupVectorizer.fit_transform">[docs]</a>    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms in ``tokenized_docs`` and, if not already provided, build up</span>
<span class="sd">        a vocabulary based those terms; do the same for the groups in ``grps``.</span>
<span class="sd">        Fit and store global weights (IDFs) and, if needed for term weighting,</span>
<span class="sd">        the average document length. Transform ``tokenized_docs`` into a</span>
<span class="sd">        group-term matrix with values weighted according to the parameters in</span>
<span class="sd">        :class:`GroupVectorizer` initialization.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">            grps (Iterable[str]): Sequence of group names by which the terms in</span>
<span class="sd">                ``tokenized_docs`` are aggregated, where the first item in ``grps``</span>
<span class="sd">                corresponds to the first item in ``tokenized_docs``, and so on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`: The transformed group-term matrix.</span>
<span class="sd">            Rows correspond to groups and columns correspond to terms.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># count terms and fit global weights</span>
        <span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">)</span>
        <span class="c1"># re-weight values in group-term matrix, as specified in init</span>
        <span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reweight_values</span><span class="p">(</span><span class="n">grp_term_matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grp_term_matrix</span></div>

<div class="viewcode-block" id="GroupVectorizer.transform"><a class="viewcode-back" href="../../../api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.GroupVectorizer.transform">[docs]</a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform ``tokenized_docs`` and ``grps`` into a group-term matrix with</span>
<span class="sd">        values weighted according to the parameters in :class:`GroupVectorizer`</span>
<span class="sd">        initialization and the global weights computed by calling</span>
<span class="sd">        :meth:`GroupVectorizer.fit()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized</span>
<span class="sd">                documents, where each is a sequence of (str) terms. For example::</span>

<span class="sd">                    &gt;&gt;&gt; ([tok.lemma_ for tok in spacy_doc]</span>
<span class="sd">                    ...  for spacy_doc in spacy_docs)</span>
<span class="sd">                    &gt;&gt;&gt; ((ne.text for ne in extract.entities(doc))</span>
<span class="sd">                    ...  for doc in corpus)</span>
<span class="sd">                    &gt;&gt;&gt; (doc._.to_terms_list(as_strings=True)</span>
<span class="sd">                    ...  for doc in docs)</span>

<span class="sd">            grps (Iterable[str]): Sequence of group names by which the terms in</span>
<span class="sd">                ``tokenized_docs`` are aggregated, where the first item in ``grps``</span>
<span class="sd">                corresponds to the first item in ``tokenized_docs``, and so on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`: The transformed group-term matrix.</span>
<span class="sd">            Rows correspond to groups and columns correspond to terms.</span>

<span class="sd">        Note:</span>
<span class="sd">            For best results, the tokenization used to produce ``tokenized_docs``</span>
<span class="sd">            should be the same as was applied to the docs used in fitting this</span>
<span class="sd">            vectorizer or in generating a fixed input vocabulary.</span>

<span class="sd">            Consider an extreme case where the docs used in fitting consist of</span>
<span class="sd">            lowercased (non-numeric) terms, while the docs to be transformed are</span>
<span class="sd">            all uppercased: The output group-term-matrix will be empty.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_vocabulary</span><span class="p">()</span>
        <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_terms</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reweight_values</span><span class="p">(</span><span class="n">grp_term_matrix</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms and, if :attr:`Vectorizer.fixed_terms` is False, build up</span>
<span class="sd">        a vocabulary based on the terms found in ``tokenized_docs``. Transform</span>
<span class="sd">        ``tokenized_docs`` into a document-term matrix with absolute tf weights.</span>
<span class="sd">        Store global weights (IDFs) and, if :attr:`Vectorizer.doc_length_norm`</span>
<span class="sd">        is not None, the average doc length.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]])</span>
<span class="sd">            grps (Iterable[str])</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># count terms and, if not provided on init, build up a vocabulary</span>
        <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span><span class="p">,</span> <span class="n">vocabulary_grps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_terms</span><span class="p">(</span>
            <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_grps</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># filter terms by doc freq or info content, as specified in init</span>
            <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_terms</span><span class="p">(</span>
                <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span>
            <span class="p">)</span>
            <span class="c1"># sort features alphabetically (vocabulary_terms modified in-place)</span>
            <span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_vocab_and_matrix</span><span class="p">(</span>
                <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span>
            <span class="p">)</span>
            <span class="c1"># *now* vocabulary_terms are known and fixed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="n">vocabulary_terms</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_terms</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_grps</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># sort groups alphabetically (vocabulary_grps modified in-place)</span>
            <span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sort_vocab_and_matrix</span><span class="p">(</span>
                <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_grps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;rows&quot;</span>
            <span class="p">)</span>
            <span class="c1"># *now* vocabulary_grps are known and fixed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span> <span class="o">=</span> <span class="n">vocabulary_grps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_grps</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">n_grps</span><span class="p">,</span> <span class="n">n_terms</span> <span class="o">=</span> <span class="n">grp_term_matrix</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_idf</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># store the global weights as a diagonal sparse matrix of idfs</span>
            <span class="n">idfs</span> <span class="o">=</span> <span class="n">get_inverse_doc_freqs</span><span class="p">(</span><span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">idf_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_idf_diag</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">spdiags</span><span class="p">(</span>
                <span class="n">idfs</span><span class="p">,</span> <span class="n">diags</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">n_terms</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_terms</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_type</span> <span class="o">==</span> <span class="s2">&quot;bm25&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dl</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># store the avg document length, used in bm25 weighting to normalize</span>
            <span class="c1"># term weights by the length of the containing documents</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_avg_doc_length</span> <span class="o">=</span> <span class="n">get_doc_lengths</span><span class="p">(</span>
                <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">type_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_type</span>
            <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">grp_term_matrix</span>

    <span class="k">def</span> <span class="nf">_count_terms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">grps</span><span class="p">,</span> <span class="n">fixed_vocab_terms</span><span class="p">,</span> <span class="n">fixed_vocab_grps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count terms and build up a vocabulary based on the terms found in the</span>
<span class="sd">        ``tokenized_docs`` and the groups found in ``grps``.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenized_docs (Iterable[Iterable[str]])</span>
<span class="sd">            grps (Iterable[str])</span>
<span class="sd">            fixed_vocab_terms (bool)</span>
<span class="sd">            fixed_vocab_grps (bool)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`scipy.sparse.csr_matrix`</span>
<span class="sd">            Dict[str, int]</span>
<span class="sd">            Dict[str, int]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: can we adapt the optimization from `Vectorizer._count_terms()` here?</span>
        <span class="k">if</span> <span class="n">fixed_vocab_terms</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># add a new value when a new term is seen</span>
            <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">()</span>
            <span class="n">vocabulary_terms</span><span class="o">.</span><span class="n">default_factory</span> <span class="o">=</span> <span class="n">vocabulary_terms</span><span class="o">.</span><span class="fm">__len__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_terms</span>

        <span class="k">if</span> <span class="n">fixed_vocab_grps</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># add a new value when a new group is seen</span>
            <span class="n">vocabulary_grps</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">()</span>
            <span class="n">vocabulary_grps</span><span class="o">.</span><span class="n">default_factory</span> <span class="o">=</span> <span class="n">vocabulary_grps</span><span class="o">.</span><span class="fm">__len__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vocabulary_grps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_grps</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">))</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">))</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">grp</span><span class="p">,</span> <span class="n">terms</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grps</span><span class="p">,</span> <span class="n">tokenized_docs</span><span class="p">):</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">grp_idx</span> <span class="o">=</span> <span class="n">vocabulary_grps</span><span class="p">[</span><span class="n">grp</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="c1"># ignore out-of-vocabulary groups when fixed_grps=True</span>
                <span class="k">continue</span>

            <span class="n">term_counter</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">terms</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">term_idx</span> <span class="o">=</span> <span class="n">vocabulary_terms</span><span class="p">[</span><span class="n">term</span><span class="p">]</span>
                    <span class="n">term_counter</span><span class="p">[</span><span class="n">term_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="c1"># ignore out-of-vocabulary terms when fixed_terms=True</span>
                    <span class="k">continue</span>

            <span class="n">data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">term_counter</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">term_counter</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">rows</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">grp_idx</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">term_counter</span><span class="p">)))</span>

        <span class="c1"># do we still want defaultdict behaviour?</span>
        <span class="k">if</span> <span class="n">fixed_vocab_terms</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">vocabulary_terms</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">vocabulary_terms</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fixed_vocab_grps</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">vocabulary_grps</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">vocabulary_grps</span><span class="p">)</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>

        <span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span>
            <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary_grps</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary_terms</span><span class="p">)),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">grp_term_matrix</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">grp_term_matrix</span><span class="p">,</span> <span class="n">vocabulary_terms</span><span class="p">,</span> <span class="n">vocabulary_grps</span></div>
</pre></div>

          </div>
              <div class="related bottom">
                &nbsp;
  <nav id="rellinks">
    <ul>
    </ul>
  </nav>
              </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../../../index.html">
    <img class="logo" src="../../../_static/textacy_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=chartbeat-labs&repo=textacy&type=watch&count=False&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_reference/root.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../changes.html">Changes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../license.html">License</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020 Chartbeat, Inc.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>