
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Vectorization &amp; Topic Modeling &#8212; textacy 0.10.0 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="IO" href="io.html" />
    <link rel="prev" title="Information Extraction" href="information_extraction.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-textacy.vsm.vectorizers">
<span id="vectorization-topic-modeling"></span><span id="ref-api-reference-vsm-and-tm"></span><h1>Vectorization &amp; Topic Modeling<a class="headerlink" href="#module-textacy.vsm.vectorizers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="vectorizers">
<h2>Vectorizers<a class="headerlink" href="#vectorizers" title="Permalink to this headline">¶</a></h2>
<p>Transform a collection of tokenized documents into a document-term matrix
of shape (# docs, # unique terms), with various ways to filter or limit
included terms and flexible weighting schemes for their values.</p>
<p>A second option aggregates terms in tokenized documents by provided group labels,
resulting in a “group-term-matrix” of shape (# unique groups, # unique terms),
with filtering and weighting functionality as described above.</p>
<p>See the <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a> and <a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer" title="textacy.vsm.vectorizers.GroupVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupVectorizer</span></code></a> docstrings for usage
examples and explanations of the various weighting schemes.</p>
<dl class="class">
<dt id="textacy.vsm.vectorizers.Vectorizer">
<em class="property">class </em><code class="sig-prename descclassname">textacy.vsm.vectorizers.</code><code class="sig-name descname">Vectorizer</code><span class="sig-paren">(</span><em class="sig-param">*</em>, <em class="sig-param">tf_type='linear'</em>, <em class="sig-param">apply_idf=False</em>, <em class="sig-param">idf_type='smooth'</em>, <em class="sig-param">apply_dl=False</em>, <em class="sig-param">dl_type='sqrt'</em>, <em class="sig-param">norm=None</em>, <em class="sig-param">min_df=1</em>, <em class="sig-param">max_df=1.0</em>, <em class="sig-param">max_n_terms=None</em>, <em class="sig-param">vocabulary_terms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#Vectorizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform one or more tokenized documents into a sparse document-term matrix
of shape (# docs, # unique terms), with flexibly weighted and normalized values.</p>
<p>Stream a corpus with metadata from disk:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ds</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CapitolWords</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">records</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">records</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">Corpus</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">records</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span>
<span class="go">Corpus(1000 docs; 538172 tokens)</span>
</pre></div>
</div>
<p>Tokenize and vectorize the first 600 documents of this corpus:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[:</span><span class="mi">600</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">apply_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span>
<span class="go">&lt;600x4346 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 69673 stored elements in Compressed Sparse Row format&gt;</span>
</pre></div>
</div>
<p>Tokenize and vectorize the remaining 400 documents of the corpus, using only
the groups, terms, and weights learned in the previous step:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">600</span><span class="p">:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span>
<span class="go">&lt;400x4346 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 38756 stored elements in Compressed Sparse Row format&gt;</span>
</pre></div>
</div>
<p>Inspect the terms associated with columns; they’re sorted alphabetically:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">terms_list</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="go">[&#39;&#39;, &#39;$&#39;, &#39;$ 1 million&#39;, &#39;$ 1.2 billion&#39;, &#39;$ 10 billion&#39;]</span>
</pre></div>
</div>
<p>(Btw: That empty string shouldn’t be there. Somehow, spaCy is labeling it as
a named entity…)</p>
<p>If known in advance, limit the terms included in vectorized outputs
to a particular set of values:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[:</span><span class="mi">600</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">apply_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">vocabulary_terms</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;president&quot;</span><span class="p">,</span> <span class="s2">&quot;bill&quot;</span><span class="p">,</span> <span class="s2">&quot;unanimous&quot;</span><span class="p">,</span> <span class="s2">&quot;distinguished&quot;</span><span class="p">,</span> <span class="s2">&quot;american&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span>
<span class="go">&lt;600x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 844 stored elements in Compressed Sparse Row format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">terms_list</span>
<span class="go">[&#39;american&#39;, &#39;bill&#39;, &#39;distinguished&#39;, &#39;president&#39;, &#39;unanimous&#39;]</span>
</pre></div>
</div>
<p>Specify different weighting schemes to determine values in the matrix,
adding or customizing individual components, as desired:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">money_idx</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_terms</span><span class="p">[</span><span class="s2">&quot;$&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span>
<span class="gp">... </span>    <span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="n">money_idx</span><span class="p">]</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="go">[[0]</span>
<span class="go"> [0]</span>
<span class="go"> [1]</span>
<span class="go"> [4]</span>
<span class="go"> [0]</span>
<span class="go"> [0]</span>
<span class="go"> [2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span> <span class="n">apply_dl</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dl_type</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span>
<span class="gp">... </span>    <span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="n">money_idx</span><span class="p">]</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="go">[[0.        ]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [0.10101525]</span>
<span class="go"> [0.26037782]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [0.11396058]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;bm25&quot;</span><span class="p">,</span> <span class="n">apply_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span>
<span class="gp">... </span>    <span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="n">money_idx</span><span class="p">]</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="go">[[0.        ]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [3.28353965]</span>
<span class="go"> [5.82763722]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [0.        ]</span>
<span class="go"> [4.83933924]]</span>
</pre></div>
</div>
<p>If you’re not sure what’s going on mathematically, <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer.weighting" title="textacy.vsm.vectorizers.Vectorizer.weighting"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Vectorizer.weighting</span></code></a>
gives the formula being used to calculate weights, based on the parameters
set when initializing the vectorizer:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">weighting</span>
<span class="go">&#39;(tf * (k + 1)) / (k + tf) * log((n_docs + 1) / (df + 1)) + 1&#39;</span>
</pre></div>
</div>
<p>In general, weights may consist of a local component (term frequency),
a global component (inverse document frequency), and a normalization
component (document length). Individual components may be modified:
they may have different scaling (e.g. tf vs. sqrt(tf)) or different behaviors
(e.g. “standard” idf vs bm25’s version). There are <em>many</em> possible weightings,
and some may be better for particular use cases than others. When in doubt,
though, just go with something standard.</p>
<ul class="simple">
<li><p>“tf”: Weights are simply the absolute per-document term frequencies (tfs),
i.e. value (i, j) in an output doc-term matrix corresponds to the number
of occurrences of term j in doc i. Terms appearing many times in a given
doc receive higher weights than less common terms.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;linear&quot;,</span> <span class="pre">apply_idf=False,</span> <span class="pre">apply_dl=False</span></code></p></li>
<li><p>“tfidf”: Doc-specific, <em>local</em> tfs are multiplied by their corpus-wide,
<em>global</em> inverse document frequencies (idfs). Terms appearing in many docs
have higher document frequencies (dfs), correspondingly smaller idfs, and
in turn, lower weights.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;linear&quot;,</span> <span class="pre">apply_idf=True,</span> <span class="pre">idf_type=&quot;smooth&quot;,</span> <span class="pre">apply_dl=False</span></code></p></li>
<li><p>“bm25”: This scheme includes a local tf component that increases asymptotically,
so higher tfs have diminishing effects on the overall weight; a global idf
component that can go <em>negative</em> for terms that appear in a sufficiently
high proportion of docs; as well as a row-wise normalization that accounts for
document length, such that terms in shorter docs hit the tf asymptote sooner
than those in longer docs.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;bm25&quot;,</span> <span class="pre">apply_idf=True,</span> <span class="pre">idf_type=&quot;bm25&quot;,</span> <span class="pre">apply_dl=True</span></code></p></li>
<li><p>“binary”: This weighting scheme simply replaces all non-zero tfs with 1,
indicating the presence or absence of a term in a particular doc. That’s it.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;binary&quot;,</span> <span class="pre">apply_idf=False,</span> <span class="pre">apply_dl=False</span></code></p></li>
</ul>
<p>Slightly altered versions of these “standard” weighting schemes are common,
and may have better behavior in general use cases:</p>
<ul class="simple">
<li><p>“lucene-style tfidf”: Adds a doc-length normalization to the usual local
and global components.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;linear&quot;,</span> <span class="pre">apply_idf=True,</span> <span class="pre">idf_type=&quot;smooth&quot;,</span> <span class="pre">apply_dl=True,</span> <span class="pre">dl_type=&quot;sqrt&quot;</span></code></p></li>
<li><p>“lucene-style bm25”: Uses a smoothed idf instead of the classic bm25 variant
to prevent weights on terms from going negative.
Params: <code class="docutils literal notranslate"><span class="pre">tf_type=&quot;bm25&quot;,</span> <span class="pre">apply_idf=True,</span> <span class="pre">idf_type=&quot;smooth&quot;,</span> <span class="pre">apply_dl=True,</span> <span class="pre">dl_type=&quot;linear&quot;</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tf_type</strong> (<em>{&quot;linear&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log&quot;</em><em>, </em><em>&quot;binary&quot;}</em>) – <p>Type of term frequency (tf)
to use for weights’ local component:</p>
<ul>
<li><p>”linear”: tf (tfs are already linear, so left as-is)</p></li>
<li><p>”sqrt”: tf =&gt; sqrt(tf)</p></li>
<li><p>”log”: tf =&gt; log(tf) + 1</p></li>
<li><p>”binary”: tf =&gt; 1</p></li>
</ul>
</p></li>
<li><p><strong>apply_idf</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, apply global idfs to local term weights, i.e.
divide per-doc term frequencies by the (log of the) total number
of documents in which they appear; otherwise, don’t.</p></li>
<li><p><strong>idf_type</strong> (<em>{&quot;standard&quot;</em><em>, </em><em>&quot;smooth&quot;</em><em>, </em><em>&quot;bm25&quot;}</em>) – <p>Type of inverse document
frequency (idf) to use for weights’ global component:</p>
<ul>
<li><p>”standard”: idf = log(n_docs / df) + 1.0</p></li>
<li><p>”smooth”: idf = log(n_docs + 1 / df + 1) + 1.0, i.e. 1 is added
to all document frequencies, as if a single document containing
every unique term was added to the corpus. This prevents zero divisions!</p></li>
<li><p>”bm25”: idf = log((n_docs - df + 0.5) / (df + 0.5)), which is
a form commonly used in information retrieval that allows for
very common terms to receive negative weights.</p></li>
</ul>
</p></li>
<li><p><strong>apply_dl</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, normalize local(+global) weights by doc length,
i.e. divide by the total number of in-vocabulary terms appearing
in a given doc; otherwise, don’t.</p></li>
<li><p><strong>dl_type</strong> (<em>{&quot;linear&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log&quot;}</em>) – <p>Type of document-length scaling
to use for weights’ normalization component:</p>
<ul>
<li><p>”linear”: dl (dls are already linear, so left as-is)</p></li>
<li><p>”sqrt”: dl =&gt; sqrt(dl)</p></li>
<li><p>”log”: dl =&gt; log(dl)</p></li>
</ul>
</p></li>
<li><p><strong>norm</strong> (<em>{&quot;l1&quot;</em><em>, </em><em>&quot;l2&quot;}</em><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) – If “l1” or “l2”, normalize weights by the
L1 or L2 norms, respectively, of row-wise vectors; otherwise, don’t.</p></li>
<li><p><strong>vocabulary_terms</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>] or </em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Mapping of unique term
string to unique term id, or an iterable of term strings that gets
converted into a suitable mapping. Note that, if specified, vectorized
outputs will include <em>only</em> these terms as columns.</p></li>
<li><p><strong>min_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – If float, value is the fractional proportion of
the total number of documents, which must be in [0.0, 1.0]. If int,
value is the absolute number. Filter terms whose document frequency
is less than <code class="docutils literal notranslate"><span class="pre">min_df</span></code>.</p></li>
<li><p><strong>max_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – If float, value is the fractional proportion of
the total number of documents, which must be in [0.0, 1.0]. If int,
value is the absolute number. Filter terms whose document frequency
is greater than <code class="docutils literal notranslate"><span class="pre">max_df</span></code>.</p></li>
<li><p><strong>max_n_terms</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Only include terms whose document frequency is within
the top <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="textacy.vsm.vectorizers.Vectorizer.vocabulary_terms">
<code class="sig-name descname">vocabulary_terms</code><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.vocabulary_terms" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique term string to unique
term id, either provided on instantiation or generated by calling
<a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer.fit" title="textacy.vsm.vectorizers.Vectorizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Vectorizer.fit()</span></code></a> on a collection of tokenized documents.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>, <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.Vectorizer.id_to_term">
<code class="sig-name descname">id_to_term</code><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.id_to_term" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique term id to unique term
string, i.e. the inverse of <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer.vocabulary_terms" title="textacy.vsm.vectorizers.Vectorizer.vocabulary_terms"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Vectorizer.vocabulary_terms</span></code></a>.
This mapping is only generated as needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>, <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.Vectorizer.terms_list">
<code class="sig-name descname">terms_list</code><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.terms_list" title="Permalink to this definition">¶</a></dt>
<dd><p>List of term strings in column order of
vectorized outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>List[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">id_to_term</code></dt>
<dd><p>Mapping of unique term id (int) to unique term string (str), i.e.
the inverse of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Vectorizer.vocabulary</span></code>. This attribute is only
generated if needed, and it is automatically kept in sync with the
corresponding vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)">dict</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">terms_list</code></dt>
<dd><p>List of term strings in column order of vectorized outputs. For example,
<code class="docutils literal notranslate"><span class="pre">terms_list[0]</span></code> gives the term assigned to the first column in an
output doc-term-matrix, <code class="docutils literal notranslate"><span class="pre">doc_term_matrix[:,</span> <span class="pre">0]</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.Vectorizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#Vectorizer.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Count terms in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> and, if not already provided, build up
a vocabulary based those terms. Fit and store global weights (IDFs)
and, if needed for term weighting, the average document length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The instance that has just been fit.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.Vectorizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#Vectorizer.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Count terms in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> and, if not already provided, build up
a vocabulary based those terms. Fit and store global weights (IDFs)
and, if needed for term weighting, the average document length.
Transform <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> into a document-term matrix with values
weighted according to the parameters in <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a> initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The transformed document-term matrix.
Rows correspond to documents and columns correspond to terms.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.Vectorizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#Vectorizer.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> into a document-term matrix with values
weighted according to the parameters in <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a> initialization
and the global weights computed by calling <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer.fit" title="textacy.vsm.vectorizers.Vectorizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Vectorizer.fit()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The transformed document-term matrix.
Rows correspond to documents and columns correspond to terms.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best results, the tokenization used to produce <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code>
should be the same as was applied to the docs used in fitting this
vectorizer or in generating a fixed input vocabulary.</p>
<p>Consider an extreme case where the docs used in fitting consist of
lowercased (non-numeric) terms, while the docs to be transformed are
all uppercased: The output doc-term-matrix will be empty.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.Vectorizer.weighting">
<em class="property">property </em><code class="sig-name descname">weighting</code><a class="headerlink" href="#textacy.vsm.vectorizers.Vectorizer.weighting" title="Permalink to this definition">¶</a></dt>
<dd><p>A mathematical representation of the overall weighting scheme
used to determine values in the vectorized matrix, depending on the
params used to initialize the <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="textacy.vsm.vectorizers.GroupVectorizer">
<em class="property">class </em><code class="sig-prename descclassname">textacy.vsm.vectorizers.</code><code class="sig-name descname">GroupVectorizer</code><span class="sig-paren">(</span><em class="sig-param">*</em>, <em class="sig-param">tf_type='linear'</em>, <em class="sig-param">apply_idf=False</em>, <em class="sig-param">idf_type='smooth'</em>, <em class="sig-param">apply_dl=False</em>, <em class="sig-param">dl_type='linear'</em>, <em class="sig-param">norm=None</em>, <em class="sig-param">min_df=1</em>, <em class="sig-param">max_df=1.0</em>, <em class="sig-param">max_n_terms=None</em>, <em class="sig-param">vocabulary_terms=None</em>, <em class="sig-param">vocabulary_grps=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#GroupVectorizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform one or more tokenized documents into a group-term matrix of
shape (# groups, # unique terms), with tf-, tf-idf, or binary-weighted values.</p>
<p>This is an extension of typical document-term matrix vectorization, where
terms are grouped by the documents in which they co-occur. It allows for
customized grouping, such as by a shared author or publication year, that
may span multiple documents, without forcing users to merge those documents
themselves.</p>
<p>Stream a corpus with metadata from disk:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ds</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CapitolWords</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">records</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">records</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">Corpus</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">records</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span>
<span class="go">Corpus(1000 docs; 538172 tokens)</span>
</pre></div>
</div>
<p>Tokenize and vectorize the first 600 documents of this corpus, where terms
are grouped not by documents but by a categorical value in the docs’ metadata:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">unzip</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">... </span>     <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;speaker_name&quot;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[:</span><span class="mi">600</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">GroupVectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">apply_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span>
<span class="go">&lt;5x1793 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 6075 stored elements in Compressed Sparse Row format&gt;</span>
</pre></div>
</div>
<p>Tokenize and vectorize the remaining 400 documents of the corpus, using only
the groups, terms, and weights learned in the previous step:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">unzip</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">... </span>     <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;speaker_name&quot;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">600</span><span class="p">:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span>
<span class="go">&lt;5x1793 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 4440 stored elements in Compressed Sparse Row format&gt;</span>
</pre></div>
</div>
<p>Inspect the terms associated with columns and groups associated with rows;
they’re sorted alphabetically:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">terms_list</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="go">[&#39;$ 1 million&#39;, &#39;$ 160 million&#39;, &#39;$ 7 billion&#39;, &#39;0&#39;, &#39;1 minute&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">grps_list</span>
<span class="go">[&#39;Bernie Sanders&#39;, &#39;John Kasich&#39;, &#39;Joseph Biden&#39;, &#39;Lindsey Graham&#39;, &#39;Rick Santorum&#39;]</span>
</pre></div>
</div>
<p>If known in advance, limit the terms and/or groups included in vectorized outputs
to a particular set of values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">unzip</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">entities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">... </span>     <span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;speaker_name&quot;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[:</span><span class="mi">600</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">GroupVectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">apply_idf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">vocabulary_terms</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;legislation&quot;</span><span class="p">,</span> <span class="s2">&quot;federal government&quot;</span><span class="p">,</span> <span class="s2">&quot;house&quot;</span><span class="p">,</span> <span class="s2">&quot;constitutional&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">vocabulary_grps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Bernie Sanders&quot;</span><span class="p">,</span> <span class="s2">&quot;Lindsey Graham&quot;</span><span class="p">,</span> <span class="s2">&quot;Rick Santorum&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grp_term_matrix</span>
<span class="go">&lt;3x4 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">        with 12 stored elements in Compressed Sparse Row format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">terms_list</span>
<span class="go">[&#39;constitutional&#39;, &#39;federal government&#39;, &#39;house&#39;, &#39;legislation&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">grps_list</span>
<span class="go">[&#39;Bernie Sanders&#39;, &#39;Lindsey Graham&#39;, &#39;Rick Santorum&#39;]</span>
</pre></div>
</div>
<p>For a discussion of the various weighting schemes that can be applied, check
out the <a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a> docstring.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tf_type</strong> (<em>{&quot;linear&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log&quot;</em><em>, </em><em>&quot;binary&quot;}</em>) – <p>Type of term frequency (tf)
to use for weights’ local component:</p>
<ul>
<li><p>”linear”: tf (tfs are already linear, so left as-is)</p></li>
<li><p>”sqrt”: tf =&gt; sqrt(tf)</p></li>
<li><p>”log”: tf =&gt; log(tf) + 1</p></li>
<li><p>”binary”: tf =&gt; 1</p></li>
</ul>
</p></li>
<li><p><strong>apply_idf</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, apply global idfs to local term weights, i.e.
divide per-doc term frequencies by the total number of documents
in which they appear (well, the log of that number); otherwise, don’t.</p></li>
<li><p><strong>idf_type</strong> (<em>{&quot;standard&quot;</em><em>, </em><em>&quot;smooth&quot;</em><em>, </em><em>&quot;bm25&quot;}</em>) – <p>Type of inverse document
frequency (idf) to use for weights’ global component:</p>
<ul>
<li><p>”standard”: idf = log(n_docs / df) + 1.0</p></li>
<li><p>”smooth”: idf = log(n_docs + 1 / df + 1) + 1.0, i.e. 1 is added
to all document frequencies, as if a single document containing
every unique term was added to the corpus.</p></li>
<li><p>”bm25”: idf = log((n_docs - df + 0.5) / (df + 0.5)), which is
a form commonly used in information retrieval that allows for
very common terms to receive negative weights.</p></li>
</ul>
</p></li>
<li><p><strong>apply_dl</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, normalize local(+global) weights by doc length,
i.e. divide by the total number of in-vocabulary terms appearing
in a given doc; otherwise, don’t.</p></li>
<li><p><strong>dl_type</strong> (<em>{&quot;linear&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log&quot;}</em>) – <p>Type of document-length scaling
to use for weights’ normalization component:</p>
<ul>
<li><p>”linear”: dl (dls are already linear, so left as-is)</p></li>
<li><p>”sqrt”: dl =&gt; sqrt(dl)</p></li>
<li><p>”log”: dl =&gt; log(dl)</p></li>
</ul>
</p></li>
<li><p><strong>norm</strong> (<em>{&quot;l1&quot;</em><em>, </em><em>&quot;l2&quot;}</em><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) – If “l1” or “l2”, normalize weights by the
L1 or L2 norms, respectively, of row-wise vectors; otherwise, don’t.</p></li>
<li><p><strong>vocabulary_terms</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>] or </em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Mapping of unique term
string to unique term id, or an iterable of term strings that gets
converted into a suitable mapping. Note that, if specified, vectorized
outputs will include <em>only</em> these terms as columns.</p></li>
<li><p><strong>vocabulary_grps</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>] or </em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Mapping of unique group
string to unique group id, or an iterable of group strings that gets
converted into a suitable mapping. Note that, if specified, vectorized
outputs will include <em>only</em> these groups as rows.</p></li>
<li><p><strong>min_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – If float, value is the fractional proportion of
the total number of documents, which must be in [0.0, 1.0]. If int,
value is the absolute number. Filter terms whose document frequency
is less than <code class="docutils literal notranslate"><span class="pre">min_df</span></code>.</p></li>
<li><p><strong>max_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – If float, value is the fractional proportion of
the total number of documents, which must be in [0.0, 1.0]. If int,
value is the absolute number. Filter terms whose document frequency
is greater than <code class="docutils literal notranslate"><span class="pre">max_df</span></code>.</p></li>
<li><p><strong>max_n_terms</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Only include terms whose document frequency is within
the top <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.vocabulary_terms">
<code class="sig-name descname">vocabulary_terms</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.vocabulary_terms" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique term string to unique
term id, either provided on instantiation or generated by calling
<a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.fit" title="textacy.vsm.vectorizers.GroupVectorizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">GroupVectorizer.fit()</span></code></a> on a collection of tokenized documents.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>, <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps">
<code class="sig-name descname">vocabulary_grps</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique group string to unique
group id, either provided on instantiation or generated by calling
<a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.fit" title="textacy.vsm.vectorizers.GroupVectorizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">GroupVectorizer.fit()</span></code></a> on a collection of tokenized documents.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>, <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.id_to_term">
<code class="sig-name descname">id_to_term</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.id_to_term" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique term id to unique term
string, i.e. the inverse of <a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.vocabulary_terms" title="textacy.vsm.vectorizers.GroupVectorizer.vocabulary_terms"><code class="xref py py-attr docutils literal notranslate"><span class="pre">GroupVectorizer.vocabulary_terms</span></code></a>.
This mapping is only generated as needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>, <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.id_to_grp">
<code class="sig-name descname">id_to_grp</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.id_to_grp" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping of unique group id to unique group
string, i.e. the inverse of <a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps" title="textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps"><code class="xref py py-attr docutils literal notranslate"><span class="pre">GroupVectorizer.vocabulary_grps</span></code></a>.
This mapping is only generated as needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a>, <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.terms_list">
<code class="sig-name descname">terms_list</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.terms_list" title="Permalink to this definition">¶</a></dt>
<dd><p>List of term strings in column order of
vectorized outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>List[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.grps_list">
<code class="sig-name descname">grps_list</code><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.grps_list" title="Permalink to this definition">¶</a></dt>
<dd><p>List of group strings in row order of
vectorized outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>List[<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#textacy.vsm.vectorizers.Vectorizer" title="textacy.vsm.vectorizers.Vectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vectorizer</span></code></a></p>
</div>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">id_to_grp</code></dt>
<dd><p>Mapping of unique group id (int) to unique group string (str), i.e.
the inverse of <a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps" title="textacy.vsm.vectorizers.GroupVectorizer.vocabulary_grps"><code class="xref py py-attr docutils literal notranslate"><span class="pre">GroupVectorizer.vocabulary_grps</span></code></a>. This attribute
is only generated if needed, and it is automatically kept in sync
with the corresponding vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)">dict</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">grps_list</code></dt>
<dd><p>List of group strings in row order of vectorized outputs. For example,
<code class="docutils literal notranslate"><span class="pre">grps_list[0]</span></code> gives the group assigned to the first row in an
output group-term-matrix, <code class="docutils literal notranslate"><span class="pre">grp_term_matrix[0,</span> <span class="pre">:]</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em>, <em class="sig-param">grps</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#GroupVectorizer.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Count terms in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> and, if not already provided, build up
a vocabulary based those terms; do the same for the groups in <code class="docutils literal notranslate"><span class="pre">grps</span></code>.
Fit and store global weights (IDFs) and, if needed for term weighting,
the average document length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p></li>
<li><p><strong>grps</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Sequence of group names by which the terms in
<code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> are aggregated, where the first item in <code class="docutils literal notranslate"><span class="pre">grps</span></code>
corresponds to the first item in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code>, and so on.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The instance that has just been fit.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer" title="textacy.vsm.vectorizers.GroupVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupVectorizer</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em>, <em class="sig-param">grps</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#GroupVectorizer.fit_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Count terms in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> and, if not already provided, build up
a vocabulary based those terms; do the same for the groups in <code class="docutils literal notranslate"><span class="pre">grps</span></code>.
Fit and store global weights (IDFs) and, if needed for term weighting,
the average document length. Transform <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> into a
group-term matrix with values weighted according to the parameters in
<a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer" title="textacy.vsm.vectorizers.GroupVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupVectorizer</span></code></a> initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p></li>
<li><p><strong>grps</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Sequence of group names by which the terms in
<code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> are aggregated, where the first item in <code class="docutils literal notranslate"><span class="pre">grps</span></code>
corresponds to the first item in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code>, and so on.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The transformed group-term matrix.
Rows correspond to groups and columns correspond to terms.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.vsm.vectorizers.GroupVectorizer.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">tokenized_docs</em>, <em class="sig-param">grps</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/vectorizers.html#GroupVectorizer.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.vectorizers.GroupVectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> and <code class="docutils literal notranslate"><span class="pre">grps</span></code> into a group-term matrix with
values weighted according to the parameters in <a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer" title="textacy.vsm.vectorizers.GroupVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupVectorizer</span></code></a>
initialization and the global weights computed by calling
<a class="reference internal" href="#textacy.vsm.vectorizers.GroupVectorizer.fit" title="textacy.vsm.vectorizers.GroupVectorizer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">GroupVectorizer.fit()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenized_docs</strong> (<em>Iterable</em><em>[</em><em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em><em>]</em>) – <p>A sequence of tokenized
documents, where each is a sequence of (str) terms. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">([</span><span class="n">tok</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">spacy_doc</span><span class="p">]</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">spacy_doc</span> <span class="ow">in</span> <span class="n">spacy_docs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">((</span><span class="n">ne</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">ne</span> <span class="ow">in</span> <span class="n">extract</span><span class="o">.</span><span class="n">entities</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">to_terms_list</span><span class="p">(</span><span class="n">as_strings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</p></li>
<li><p><strong>grps</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – Sequence of group names by which the terms in
<code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code> are aggregated, where the first item in <code class="docutils literal notranslate"><span class="pre">grps</span></code>
corresponds to the first item in <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code>, and so on.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The transformed group-term matrix.
Rows correspond to groups and columns correspond to terms.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For best results, the tokenization used to produce <code class="docutils literal notranslate"><span class="pre">tokenized_docs</span></code>
should be the same as was applied to the docs used in fitting this
vectorizer or in generating a fixed input vocabulary.</p>
<p>Consider an extreme case where the docs used in fitting consist of
lowercased (non-numeric) terms, while the docs to be transformed are
all uppercased: The output group-term-matrix will be empty.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<span class="target" id="module-textacy.vsm.matrix_utils"></span><div class="section" id="sparse-matrix-utils">
<h2>Sparse Matrix Utils<a class="headerlink" href="#sparse-matrix-utils" title="Permalink to this headline">¶</a></h2>
<p>Functions for computing corpus-wide term- or document-based values, like
term frequency, document frequency, and document length, and filtering terms
from a matrix by their document frequency.</p>
<dl class="function">
<dt id="textacy.vsm.matrix_utils.get_term_freqs">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">get_term_freqs</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">type_='linear'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#get_term_freqs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.get_term_freqs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute frequencies for all terms in a document-term matrix, with optional
sub-linear scaling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M x N sparse matrix,
where M is the # of docs and N is the # of unique terms. Values must be
the linear, un-scaled counts of term n per doc m.</p></li>
<li><p><strong>type_</strong> (<em>{'linear'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log'}</em>) – Scaling applied to absolute term counts.
If ‘linear’, term counts are left as-is, since the sums are already
linear; if ‘sqrt’, tf =&gt; sqrt(tf); if ‘log’, tf =&gt; log(tf) + 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of term frequencies, with length equal to
the # of unique terms (# of columns) in <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code> doesn’t have any non-zero entries, or
    if <code class="docutils literal notranslate"><span class="pre">type_</span></code> isn’t one of {“linear”, “sqrt”, “log”}.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.get_doc_freqs">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">get_doc_freqs</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#get_doc_freqs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.get_doc_freqs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute document frequencies for all terms in a document-term matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – <p>M x N sparse matrix,
where M is the # of docs and N is the # of unique terms.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Weighting on the terms doesn’t matter! Could be binary or
tf or tfidf, a term’s doc freq will be the same.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of document frequencies, with length equal to
the # of unique terms (# of columns) in <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code> doesn’t have any non-zero entries.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.get_inverse_doc_freqs">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">get_inverse_doc_freqs</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">type_='smooth'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#get_inverse_doc_freqs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.get_inverse_doc_freqs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute inverse document frequencies for all terms in a document-term matrix,
using one of several IDF formulations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M x N sparse matrix,
where M is the # of docs and N is the # of unique terms.
The particular weighting of matrix values doesn’t matter.</p></li>
<li><p><strong>type_</strong> (<em>{'standard'</em><em>, </em><em>'smooth'</em><em>, </em><em>'bm25'}</em>) – Type of IDF formulation to use.
If ‘standard’, idfs =&gt; log(n_docs / dfs) + 1.0;
if ‘smooth’, idfs =&gt; log(n_docs + 1 / dfs + 1) + 1.0, i.e. 1 is added
to all document frequencies, equivalent to adding a single document
to the corpus containing every unique term;
if ‘bm25’, idfs =&gt; log((n_docs - dfs + 0.5) / (dfs + 0.5)), which is
a form commonly used in BM25 ranking that allows for extremely common
terms to have negative idf weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of inverse document frequencies, with length
equal to the # of unique terms (# of columns) in <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">type_</span></code> isn’t one of {“standard”, “smooth”, “bm25”}.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.get_doc_lengths">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">get_doc_lengths</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">type_='linear'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#get_doc_lengths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.get_doc_lengths" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the lengths (i.e. number of terms) for all documents in a
document-term matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M x N sparse matrix,
where M is the # of docs, N is the # of unique terms, and values are
the absolute counts of term n per doc m.</p></li>
<li><p><strong>type_</strong> (<em>{'linear'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log'}</em>) – Scaling applied to absolute doc lengths.
If ‘linear’, lengths are left as-is, since the sums are already
linear; if ‘sqrt’, dl =&gt; sqrt(dl); if ‘log’, dl =&gt; log(dl) + 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of document lengths, with length equal to
the # of documents (# of rows) in <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">type_</span></code> isn’t one of {“linear”, “sqrt”, “log”}.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.get_information_content">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">get_information_content</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#get_information_content"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.get_information_content" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute information content for all terms in a document-term matrix. IC is a
float in [0.0, 1.0], defined as <code class="docutils literal notranslate"><span class="pre">-df</span> <span class="pre">*</span> <span class="pre">log2(df)</span> <span class="pre">-</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">df)</span> <span class="pre">*</span> <span class="pre">log2(1</span> <span class="pre">-</span> <span class="pre">df)</span></code>,
where df is a term’s normalized document frequency.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – <p>M x N sparse matrix,
where M is the # of docs and N is the # of unique terms.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Weighting on the terms doesn’t matter! Could be binary or
tf or tfidf, a term’s information content will be the same.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of term information content values, with
length equal to the # of unique terms (# of columns) in <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">doc_term_matrix</span></code> doesn’t have any non-zero entries.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.apply_idf_weighting">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">apply_idf_weighting</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">type_='smooth'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#apply_idf_weighting"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.apply_idf_weighting" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply inverse document frequency (idf) weighting to a term-frequency (tf)
weighted document-term matrix, using one of several IDF formulations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M x N sparse matrix,
where M is the # of docs and N is the # of unique terms.</p></li>
<li><p><strong>type_</strong> (<em>{'standard'</em><em>, </em><em>'smooth'</em><em>, </em><em>'bm25'}</em>) – Type of IDF formulation to use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sparse matrix of shape M x N,
where value (i, j) is the tfidf weight of term j in doc i.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#textacy.vsm.matrix_utils.get_inverse_doc_freqs" title="textacy.vsm.matrix_utils.get_inverse_doc_freqs"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_inverse_doc_freqs()</span></code></a></p>
</div>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.filter_terms_by_df">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">filter_terms_by_df</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">term_to_id</em>, <em class="sig-param">*</em>, <em class="sig-param">max_df=1.0</em>, <em class="sig-param">min_df=1</em>, <em class="sig-param">max_n_terms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#filter_terms_by_df"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.filter_terms_by_df" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out terms that are too common and/or too rare (by document frequency),
and compactify the top <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code> in the <code class="docutils literal notranslate"><span class="pre">id_to_term</span></code> mapping accordingly.
Borrows heavily from the <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text</span></code> module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M X N matrix, where
M is the # of docs and N is the # of unique terms.</p></li>
<li><p><strong>term_to_id</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – Mapping of term string to unique term id,
e.g. <code class="xref py py-attr docutils literal notranslate"><span class="pre">Vectorizer.vocabulary_terms</span></code>.</p></li>
<li><p><strong>min_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – if float, value is the fractional proportion of
the total number of documents and must be in [0.0, 1.0]; if int,
value is the absolute number; filter terms whose document frequency
is less than <code class="docutils literal notranslate"><span class="pre">min_df</span></code></p></li>
<li><p><strong>max_df</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – if float, value is the fractional proportion of
the total number of documents and must be in [0.0, 1.0]; if int,
value is the absolute number; filter terms whose document frequency
is greater than <code class="docutils literal notranslate"><span class="pre">max_df</span></code></p></li>
<li><p><strong>max_n_terms</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – only include terms whose <em>term</em> frequency is within
the top <cite>max_n_terms</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Sparse matrix of shape (# docs, # unique filtered terms),
where value (i, j) is the weight of term j in doc i.</p>
<p>Dict[str, int]: Term to id mapping, where keys are unique <em>filtered</em> terms
as strings and values are their corresponding integer ids.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">max_df</span></code> or <code class="docutils literal notranslate"><span class="pre">min_df</span></code> or <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code> &lt; 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="textacy.vsm.matrix_utils.filter_terms_by_ic">
<code class="sig-prename descclassname">textacy.vsm.matrix_utils.</code><code class="sig-name descname">filter_terms_by_ic</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">term_to_id</em>, <em class="sig-param">*</em>, <em class="sig-param">min_ic=0.0</em>, <em class="sig-param">max_n_terms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/vsm/matrix_utils.html#filter_terms_by_ic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.vsm.matrix_utils.filter_terms_by_ic" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out terms that are too common and/or too rare (by information content),
and compactify the top <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code> in the <code class="docutils literal notranslate"><span class="pre">id_to_term</span></code> mapping accordingly.
Borrows heavily from the <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text</span></code> module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a>) – M X N sparse matrix,
where M is the # of docs and N is the # of unique terms.</p></li>
<li><p><strong>term_to_id</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – Mapping of term string to unique term id,
e.g. <code class="xref py py-attr docutils literal notranslate"><span class="pre">Vectorizer.vocabulary_terms</span></code>.</p></li>
<li><p><strong>min_ic</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – filter terms whose information content is less than this
value; must be in [0.0, 1.0]</p></li>
<li><p><strong>max_n_terms</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – only include terms whose information content is within
the top <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Sparse matrix of shape (# docs, # unique filtered terms),
where value (i, j) is the weight of term j in doc i.</p>
<p>Dict[str, int]: Term to id mapping, where keys are unique <em>filtered</em> terms
as strings and values are their corresponding integer ids.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" title="(in SciPy v1.4.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.csr_matrix</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">min_ic</span></code> not in [0.0, 1.0] or <code class="docutils literal notranslate"><span class="pre">max_n_terms</span></code> &lt; 0.</p>
</dd>
</dl>
</dd></dl>

</div>
<span class="target" id="module-textacy.tm.topic_model"></span><div class="section" id="topic-models">
<h2>Topic Models<a class="headerlink" href="#topic-models" title="Permalink to this headline">¶</a></h2>
<p>Convenient and consolidated topic-modeling, built on <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<dl class="class">
<dt id="textacy.tm.topic_model.TopicModel">
<em class="property">class </em><code class="sig-prename descclassname">textacy.tm.topic_model.</code><code class="sig-name descname">TopicModel</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">n_topics=10</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Train and apply a topic model to vectorized texts using scikit-learn’s
implementations of LSA, LDA, and NMF models. Also any other topic model implementations that have
<cite>component_</cite>, <cite>n_topics</cite> and <cite>transform</cite> attributes. Inspect and visualize results.
Save and load trained models to and from disk.</p>
<p>Prepare a vectorized corpus (i.e. document-term matrix) and corresponding
vocabulary (i.e. mapping of term strings to column indices in the matrix).
See <code class="xref py py-class docutils literal notranslate"><span class="pre">textacy.vsm.Vectorizer</span></code> for details. In short:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tf_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">apply_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">idf_type</span><span class="o">=</span><span class="s2">&quot;smooth&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_df</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">max_n_terms</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">terms_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Initialize and train a topic model:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">textacy</span><span class="o">.</span><span class="n">tm</span><span class="o">.</span><span class="n">TopicModel</span><span class="p">(</span><span class="s2">&quot;nmf&quot;</span><span class="p">,</span> <span class="n">n_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span>
<span class="go">TopicModel(n_topics=10, model=NMF)</span>
</pre></div>
</div>
<p>Transform the corpus and interpret our model:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc_topic_matrix</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">topic_idx</span><span class="p">,</span> <span class="n">top_terms</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">top_topic_terms</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">id_to_term</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;topic&quot;</span><span class="p">,</span> <span class="n">topic_idx</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="s2">&quot;   &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">top_terms</span><span class="p">))</span>
<span class="go">topic 0 : people   american   go   year   work   think   $   today   money   america</span>
<span class="go">topic 1 : rescind   quorum   order   unanimous   consent   ask   president   mr.   madam   absence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">topic_idx</span><span class="p">,</span> <span class="n">top_docs</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">top_topic_docs</span><span class="p">(</span><span class="n">doc_topic_matrix</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">topic_idx</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">top_docs</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">])</span>
<span class="go">0</span>
<span class="go">THE MOST IMPORTANT ISSUES FACING THE AMERICAN PEOPLE</span>
<span class="go">55TH ANNIVERSARY OF THE BATTLE OF CRETE</span>
<span class="go">1</span>
<span class="go">CHEMICAL WEAPONS CONVENTION</span>
<span class="go">MFN STATUS FOR CHINA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">doc_idx</span><span class="p">,</span> <span class="n">topics</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">top_doc_topics</span><span class="p">(</span><span class="n">doc_topic_matrix</span><span class="p">,</span> <span class="n">docs</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">]</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">],</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">topics</span><span class="p">)</span>
<span class="go">JOIN THE SENATE AND PASS A CONTINUING RESOLUTION : (9, 0)</span>
<span class="go">MEETING THE CHALLENGE : (2, 0)</span>
<span class="go">DISPOSING OF SENATE AMENDMENT TO H.R. 1643, EXTENSION OF MOST-FAVORED- NATION TREATMENT FOR BULGARIA : (0, 9)</span>
<span class="go">EXAMINING THE SPEAKER&#39;S UPCOMING TRAVEL SCHEDULE : (0, 9)</span>
<span class="go">FLOODING IN PENNSYLVANIA : (0, 9)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">topic_weights</span><span class="p">(</span><span class="n">doc_topic_matrix</span><span class="p">)):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="go">0 0.302796022302</span>
<span class="go">1 0.0635617650602</span>
<span class="go">2 0.0744927472417</span>
<span class="go">3 0.0905778808867</span>
<span class="go">4 0.0521162262192</span>
<span class="go">5 0.0656303769725</span>
<span class="go">6 0.0973516532757</span>
<span class="go">7 0.112907245542</span>
<span class="go">8 0.0680659204364</span>
<span class="go">9 0.0725001620636</span>
</pre></div>
</div>
<p>Visualize the model:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">termite_plot</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">id_to_term</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="n">topics</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>  <span class="n">n_terms</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">sort_terms_by</span><span class="o">=</span><span class="s2">&quot;seriation&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Persist our topic model to disk:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;nmf-10topics.pkl&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> ({“nmf”, “lda”, “lsa”} or <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition.&lt;model&gt;</span></code>) – </p></li>
<li><p><strong>n_topics</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – number of topics in the model to be initialized</p></li>
<li><p><strong>**kwargs</strong> – variety of parameters used to initialize the model; see individual
sklearn pages for full details</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if <code class="docutils literal notranslate"><span class="pre">model</span></code> not in <code class="docutils literal notranslate"><span class="pre">{&quot;nmf&quot;,</span> <span class="pre">&quot;lda&quot;,</span> <span class="pre">&quot;lsa&quot;}</span></code> or is not an
    NMF, LatentDirichletAllocation, or TruncatedSVD instance</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html</a></p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a></p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</a></p></li>
</ul>
</div>
<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.get_doc_topic_matrix">
<code class="sig-name descname">get_doc_topic_matrix</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">normalize=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.get_doc_topic_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.get_doc_topic_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a document-term matrix into a document-topic matrix, where rows
correspond to documents and columns to the topics in the topic model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<em>array-like</em><em> or </em><em>sparse matrix</em>) – Corpus represented as a
document-term matrix with shape (n_docs, n_terms). LDA expects
tf-weighting, while NMF and LSA may do better with tfidf-weighting.</p></li>
<li><p><strong>normalize</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – if True, the values in each row are normalized,
i.e. topic weights on each document sum to 1</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Document-topic matrix with shape (n_docs, n_topics).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.top_topic_terms">
<code class="sig-name descname">top_topic_terms</code><span class="sig-paren">(</span><em class="sig-param">id2term</em>, <em class="sig-param">*</em>, <em class="sig-param">topics=-1</em>, <em class="sig-param">top_n=10</em>, <em class="sig-param">weights=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.top_topic_terms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.top_topic_terms" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> terms by weight per topic in <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>id2term</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>) or </em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – object that returns the term string corresponding
to term id <code class="docutils literal notranslate"><span class="pre">i</span></code> through <code class="docutils literal notranslate"><span class="pre">id2term[i]</span></code>; could be a list of strings
where the index represents the term id, such as that returned by
<code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer.get_feature_names()</span></code>,
or a mapping of term id: term string</p></li>
<li><p><strong>topics</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – topic(s) for which to return top terms;
if -1 (default), all topics’ terms are returned</p></li>
<li><p><strong>top_n</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – number of top terms to return per topic</p></li>
<li><p><strong>weights</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – if True, terms are returned with their corresponding
topic weights; otherwise, terms are returned without weights</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Tuple[int, Tuple[str]] or Tuple[int, Tuple[Tuple[str, float]]]</em> –     next tuple corresponding to a topic; the first element is the topic’s
index; if <code class="docutils literal notranslate"><span class="pre">weights</span></code> is False, the second element is a tuple of str
representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> related terms; otherwise, the second
is a tuple of (str, float) pairs representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code>
related terms and their associated weights wrt the topic; for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_topic_terms</span><span class="p">(</span><span class="n">id2term</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">[(0, (&#39;foo&#39;, &#39;bar&#39;)), (1, (&#39;bat&#39;, &#39;baz&#39;))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_topic_terms</span><span class="p">(</span><span class="n">id2term</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">[(0, ((&#39;foo&#39;, 0.1415), (&#39;bar&#39;, 0.0986)))]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.top_topic_docs">
<code class="sig-name descname">top_topic_docs</code><span class="sig-paren">(</span><em class="sig-param">doc_topic_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">topics=-1</em>, <em class="sig-param">top_n=10</em>, <em class="sig-param">weights=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.top_topic_docs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.top_topic_docs" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> docs by weight per topic in <code class="docutils literal notranslate"><span class="pre">doc_topic_matrix</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_topic_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – document-topic matrix with shape
(n_docs, n_topics), the result of calling <a class="reference internal" href="#textacy.tm.topic_model.TopicModel.get_doc_topic_matrix" title="textacy.tm.topic_model.TopicModel.get_doc_topic_matrix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TopicModel.get_doc_topic_matrix()</span></code></a></p></li>
<li><p><strong>topics</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – topic(s) for which to return top docs;
if -1, all topics’ docs are returned</p></li>
<li><p><strong>top_n</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – number of top docs to return per topic</p></li>
<li><p><strong>weights</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – if True, docs are returned with their corresponding
(normalized) topic weights; otherwise, docs are returned without weights</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Tuple[int, Tuple[int]] or Tuple[int, Tuple[Tuple[int, float]]]</em> –     next tuple corresponding to a topic; the first element is the topic’s
index; if <code class="docutils literal notranslate"><span class="pre">weights</span></code> is False, the second element is a tuple of ints
representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> related docs; otherwise, the second
is a tuple of (int, float) pairs representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code>
related docs and their associated weights wrt the topic; for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_doc_terms</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">[(0, (4, 2)), (1, (1, 3))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_doc_terms</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">[(0, ((4, 0.3217), (2, 0.2154)))]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.top_doc_topics">
<code class="sig-name descname">top_doc_topics</code><span class="sig-paren">(</span><em class="sig-param">doc_topic_matrix</em>, <em class="sig-param">*</em>, <em class="sig-param">docs=-1</em>, <em class="sig-param">top_n=3</em>, <em class="sig-param">weights=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.top_doc_topics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.top_doc_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> topics by weight per doc for <code class="docutils literal notranslate"><span class="pre">docs</span></code> in <code class="docutils literal notranslate"><span class="pre">doc_topic_matrix</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_topic_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – document-topic matrix with shape
(n_docs, n_topics), the result of calling <a class="reference internal" href="#textacy.tm.topic_model.TopicModel.get_doc_topic_matrix" title="textacy.tm.topic_model.TopicModel.get_doc_topic_matrix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TopicModel.get_doc_topic_matrix()</span></code></a></p></li>
<li><p><strong>docs</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – docs for which to return top topics;
if -1, all docs’ top topics are returned</p></li>
<li><p><strong>top_n</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – number of top topics to return per doc</p></li>
<li><p><strong>weights</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – if True, docs are returned with their corresponding
(normalized) topic weights; otherwise, docs are returned without weights</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Tuple[int, Tuple[int]] or Tuple[int, Tuple[Tuple[int, float]]]</em> –     next tuple corresponding to a doc; the first element is the doc’s
index; if <code class="docutils literal notranslate"><span class="pre">weights</span></code> is False, the second element is a tuple of ints
representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code> related topics; otherwise, the second
is a tuple of (int, float) pairs representing the top <code class="docutils literal notranslate"><span class="pre">top_n</span></code>
related topics and their associated weights wrt the doc; for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_doc_topics</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">docs</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">[(0, (1, 4)), (1, (3, 2))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TopicModel</span><span class="o">.</span><span class="n">top_doc_topics</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">docs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">[(0, ((1, 0.2855), (4, 0.2412)))]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.topic_weights">
<code class="sig-name descname">topic_weights</code><span class="sig-paren">(</span><em class="sig-param">doc_topic_matrix</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.topic_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.topic_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the overall weight of topics across an entire corpus. Note: Values depend
on whether topic weights per document in <code class="docutils literal notranslate"><span class="pre">doc_topic_matrix</span></code> were normalized,
or not. I suppose either way makes sense… o_O</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>doc_topic_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – document-topic matrix with shape
(n_docs, n_topics), the result of calling <a class="reference internal" href="#textacy.tm.topic_model.TopicModel.get_doc_topic_matrix" title="textacy.tm.topic_model.TopicModel.get_doc_topic_matrix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TopicModel.get_doc_topic_matrix()</span></code></a></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the ith element is the ith topic’s overall weight</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="textacy.tm.topic_model.TopicModel.termite_plot">
<code class="sig-name descname">termite_plot</code><span class="sig-paren">(</span><em class="sig-param">doc_term_matrix</em>, <em class="sig-param">id2term</em>, <em class="sig-param">*</em>, <em class="sig-param">topics=-1</em>, <em class="sig-param">sort_topics_by='index'</em>, <em class="sig-param">highlight_topics=None</em>, <em class="sig-param">n_terms=25</em>, <em class="sig-param">rank_terms_by='topic_weight'</em>, <em class="sig-param">sort_terms_by='seriation'</em>, <em class="sig-param">save=False</em>, <em class="sig-param">rc_params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/textacy/tm/topic_model.html#TopicModel.termite_plot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#textacy.tm.topic_model.TopicModel.termite_plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a “termite” plot for assessing topic models using a tabular layout
to promote comparison of terms both within and across topics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>doc_term_matrix</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a> or sparse matrix) – corpus
represented as a document-term matrix with shape (n_docs, n_terms);
may have tf- or tfidf-weighting</p></li>
<li><p><strong>id2term</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>] or </em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – object that returns the term string corresponding
to term id <code class="docutils literal notranslate"><span class="pre">i</span></code> through <code class="docutils literal notranslate"><span class="pre">id2term[i]</span></code>; could be a list of strings
where the index represents the term id, such as that returned by
<code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer.get_feature_names()</span></code>,
or a mapping of term id: term string</p></li>
<li><p><strong>topics</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – topic(s) to include in termite plot;
if -1, all topics are included</p></li>
<li><p><strong>sort_topics_by</strong> (<em>{'index'</em><em>, </em><em>'weight'}</em>) – </p></li>
<li><p><strong>highlight_topics</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>Sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – indices for up to 6 topics
to visually highlight in the plot with contrasting colors</p></li>
<li><p><strong>n_terms</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – number of top terms to include in termite plot</p></li>
<li><p><strong>rank_terms_by</strong> (<em>{'topic_weight'</em><em>, </em><em>'corpus_weight'}</em>) – value used
to rank terms; the top-ranked <code class="docutils literal notranslate"><span class="pre">n_terms</span></code> are included in the plot</p></li>
<li><p><strong>sort_terms_by</strong> (<em>{'seriation'</em><em>, </em><em>'weight'</em><em>, </em><em>'index'</em><em>, </em><em>'alphabetical'}</em>) – method used to vertically sort the selected top <code class="docutils literal notranslate"><span class="pre">n_terms</span></code> terms;
the default (“seriation”) groups similar terms together, which
facilitates cross-topic assessment</p></li>
<li><p><strong>save</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – give the full /path/to/fname on disk to save figure
rc_params (dict, optional): allow passing parameters to rc_context in matplotlib.plyplot,
details in <a class="reference external" href="https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.rc_context.html">https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.rc_context.html</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Axis on which termite plot is plotted.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">matplotlib.axes.Axes.axis</span></code></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3.6/library/exceptions.html#ValueError" title="(in Python v3.6)"><strong>ValueError</strong></a> – if more than 6 topics are selected for highlighting, or
    an invalid value is passed for the sort_topics_by, rank_terms_by,
    and/or sort_terms_by params</p>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Chuang, Jason, Christopher D. Manning, and Jeffrey Heer. “Termite:
Visualization techniques for assessing textual topic models.”
Proceedings of the International Working Conference on Advanced
Visual Interfaces. ACM, 2012.</p></li>
<li><p>for sorting by “seriation”, see <a class="reference external" href="https://arxiv.org/abs/1406.5370">https://arxiv.org/abs/1406.5370</a></p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">viz.termite_plot</span></code></p>
</div>
<p>TODO: <cite>rank_terms_by</cite> other metrics, e.g. topic salience or relevance</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
              <div class="related bottom">
                &nbsp;
  <nav id="rellinks">
    <ul>
        <li>
          &larr;
          <a href="information_extraction.html" title="Previous document">Information Extraction</a>
        </li>
        <li>
          <a href="io.html" title="Next document">IO</a>
          &rarr;
        </li>
    </ul>
  </nav>
              </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/textacy_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=chartbeat-labs&repo=textacy&type=watch&count=False&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="root.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lang_doc_corpus.html">Lang, Doc, Corpus</a></li>
<li class="toctree-l2"><a class="reference internal" href="spacier.html">spaCy extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="resources.html">Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="text_processing.html">Text (Pre-)Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="information_extraction.html">Information Extraction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Vectorization &amp; Topic Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="io.html">IO</a></li>
<li class="toctree-l2"><a class="reference internal" href="viz.html">Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="augmentation.html">Data Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="misc.html">Miscellany</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changes.html">Changes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="root.html">API Reference</a><ul>
      <li>Previous: <a href="information_extraction.html" title="previous chapter">Information Extraction</a></li>
      <li>Next: <a href="io.html" title="next chapter">IO</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020 Chartbeat, Inc.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/api_reference/vsm_and_tm.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>